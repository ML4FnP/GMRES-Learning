{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of NN implementation details for 2D problem \n",
    "\n",
    "- No changes to training strategy as established in \"OnlineImplementation.ipynb\" \n",
    "\n",
    "\n",
    "- Have found both time to solution and  n$th$ iteration error to be good metrics for adding data to training set\n",
    "\n",
    "\n",
    "- Can use fewer number of Epochs in training set up\n",
    "\n",
    "\n",
    "- Altered pytorch strutures to deal with 2D inputs. (Can also use 1D data to work with 2D problems, see final point)\n",
    "\n",
    "\n",
    "- Can continue to ensure \"spread\" of data by checking orthogonality of flattended 2D solutions\n",
    "\n",
    "\n",
    "- Identified 2 neural network architetectures that work exceptionally well "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modification to Data Filter for Adding Data to Training Set \n",
    "\n",
    "- Originally, the only metric that was used to add data to the training set was the time-to-solution. This worked well since the time-to-solution encodes information regarding the rate of convergence that the error of a single iteration does not. However, on a distributed system, run time for the same problem can vary depending on system loads. To fix this issue, we add an additional filter that depends on the error of an iterate.  Now, we essentially only add data to the training set when the time-to-solution is above average **AND** the $n$th iterate error is above average. This additional situation prevents the addition of data to the training set that is converging faster than average, but the time-to-solution decreased below average due to system loads. **The key change is bolded in the code below:**\n",
    "\n",
    "\n",
    "\n",
    "            Initial_set=5\n",
    "            IterTime_AVG=0.0\n",
    "            IterErr10_AVG=0.0\n",
    "            \n",
    "            # Check if we are in first GMRES e1 tolerance run. If so, we compute prediction, and check the prediction is \"good\" before moving forward. \n",
    "            if func.predictor.is_trained and refine==False:\n",
    "                pred_x0 = func.predictor.predict(b_flat)\n",
    "                target_test=GMRES(A, b, x0, e, 2,1, False)\n",
    "                IterErr_test = resid(A, target_test, b)\n",
    "                print('size',len(IterErr_test))\n",
    "                print(IterErr_test[-1],max(Err_list))\n",
    "                if (IterErr_test[-1]>max(Err_list)): \n",
    "                    print('poor prediction,using initial x0')\n",
    "                    pred_x0 = x0\n",
    "            else:\n",
    "                pred_x0 = x0\n",
    "\n",
    "\n",
    "            #Time GMRES function \n",
    "            tic = time.perf_counter()\n",
    "            target  = func(A, b,b_flat, pred_x0, e, nmax_iter,ML_GMRES_Time_list,ProbCount,restart,debug,refine,blist,reslist,Err_list,ML_GMRES_Time_list2, *eargs)\n",
    "            toc = time.perf_counter()\n",
    "\n",
    "            res = target[-1]\n",
    "            res_flat=np.reshape(res.T,(1,-1),order='C').squeeze(0)\n",
    "\n",
    "\n",
    "            # Check if we are in first e tolerance loop\n",
    "            if refine==False :\n",
    "                IterErr = resid(A, target, b)\n",
    "                ML_GMRES_Time_list.append((toc-tic))\n",
    "                Err_list.append(IterErr[2])  \n",
    "                if ProbCount<=Initial_set:\n",
    "                    func.predictor.add_init(b_flat, res_flat)\n",
    "                if ProbCount==Initial_set:\n",
    "                    timeLoop=func.predictor.retrain_timed()\n",
    "                    print('Initial Training')\n",
    "            else :\n",
    "                ML_GMRES_Time_list2.append((toc-tic))\n",
    "\n",
    "\n",
    "            # Compute moving averages used to filter data\n",
    "            if ProbCount>Initial_set:\n",
    "                IterTime_AVG=moving_average(np.asarray(ML_GMRES_Time_list),ProbCount)\n",
    "                IterErr10_AVG=moving_average(np.asarray(Err_list),ProbCount)\n",
    "                print(ML_GMRES_Time_list[-1],IterTime_AVG,Err_list[-1],IterErr10_AVG)\n",
    "\n",
    "\n",
    "            # Filter for data to be added to training set\n",
    "######            if (ML_GMRES_Time_list[-1]>IterTime_AVG and Err_list[-1]>IterErr10_AVG ) and  refine==True and ProbCount>Initial_set : \n",
    "                \n",
    "\n",
    "                blist.append(b_flat)\n",
    "                reslist.append(res_flat)\n",
    "                \n",
    "                # check orthogonality of 3 solutions that met training set critera\n",
    "                if   len(blist)==3 :\n",
    "                    resMat=np.asarray(reslist)\n",
    "                    resMat_square=resMat**2\n",
    "                    row_sums = resMat_square.sum(axis=1,keepdims=True)\n",
    "                    resMat= resMat/np.sqrt(row_sums)\n",
    "                    InnerProd=np.dot(resMat,resMat.T)\n",
    "                    print('InnerProd',InnerProd)\n",
    "                    func.predictor.add(np.asarray(blist[0]), np.asarray(reslist[0]))\n",
    "                    cutoff=0.8\n",
    "                    \n",
    "                    # Picking out sufficiently orthogonal subset of 3 solutions gathered\n",
    "                    if np.abs(InnerProd[0,1]) and np.abs(InnerProd[0,2])<cutoff :\n",
    "                        if np.abs(InnerProd[1,2])<cutoff :\n",
    "                            func.predictor.add(np.asarray(blist[1]), np.asarray(reslist[1]))\n",
    "                            func.predictor.add(np.asarray(blist[2]), np.asarray(reslist[2]))\n",
    "                        elif np.abs(InnerProd[1,2])>=cutoff: \n",
    "                            func.predictor.add(np.asarray(blist[1]), np.asarray(reslist[1]))\n",
    "                    elif np.abs(InnerProd[0,1])<cutoff :\n",
    "                        func.predictor.add(np.asarray(blist[1]), np.asarray(reslist[1]))\n",
    "                    elif np.abs(InnerProd[0,2])<cutoff :\n",
    "                        func.predictor.add(np.asarray(blist[2]), np.asarray(reslist[2]))\n",
    "                    \n",
    "                    if func.predictor.counter>=retrain_freq:\n",
    "                        if func.debug:\n",
    "                            print(\"retraining\")\n",
    "                            print(func.predictor.counter)\n",
    "                            timeLoop=func.predictor.retrain_timed()\n",
    "                            trainTime=float(timeLoop[-1])\n",
    "                            blist=[]\n",
    "                            reslist=[]\n",
    "            return target,ML_GMRES_Time_list,trainTime,blist,reslist,Err_list,ML_GMRES_Time_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjustments to Training Loop Pytorch Tensors \n",
    "\n",
    "- The main change to the training loop is that the number of Epochs has been reduced from 2000 to 500 with excellent results. Furthermore, the pytorch tensors are now structured with the following shapes (N,n_x,n_y) where N is the batch size, $n_x$ is the number of x-grid points, and $n_y$ is the number of y grid points. These changes have been consistently implemented in the relevant code so that 2D data can be handled correctly. However we note here that the data can also be treated using a fully flattened representation throughout. We do this as well with one of the neural network architectures we have developed. \n",
    "\n",
    " def retrain_timed(self):\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.xNew = self.xNew.to(device)\n",
    "        self.yNew = self.yNew.to(device)\n",
    "\n",
    "        self.loss_val = list()  # clear loss val history\n",
    "        self.loss_val.append(10.0)\n",
    "\n",
    "        batch_size=32\n",
    "#####        numEpochs=500\n",
    "        e1=1e-3\n",
    "        epoch=0\n",
    "        \n",
    "        while self.loss_val[-1]> e1 and epoch<numEpochs:\n",
    "            permutation = torch.randperm(self.x.size()[0])\n",
    "            for t in range(0,self.x.size()[0],batch_size):\n",
    "                \n",
    "                indices = permutation[t:t+batch_size]\n",
    "\n",
    "                batch_x, batch_y = self.x[indices],self.y[indices]\n",
    "\n",
    "                # Adding new data to each batch\n",
    "                # Note: only adding at most 3 data points to each batch\n",
    "                batch_xMix=torch.cat((batch_x,self.xNew)) \n",
    "                batch_yMix=torch.cat((batch_y,self.yNew))\n",
    "\n",
    "                # Forward pass: Compute predicted y by passing x to the model\n",
    "                y_pred = self.model(batch_xMix)\n",
    "\n",
    "                # Compute and print loss\n",
    "                loss = self.criterion(y_pred, batch_yMix)\n",
    "                self.loss_val.append(loss.item())\n",
    "\n",
    "                # Zero gradients, perform a backward pass, and update the weights.\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch=epoch+1\n",
    "                \n",
    "        print('Final loss:',loss.item())\n",
    "        self.loss_val.append(loss.item())\n",
    "\n",
    "        self.x=torch.cat((self.x,self.xNew))\n",
    "        self.y=torch.cat((self.y,self.yNew))\n",
    "#####        self.xNew = torch.empty(0, self.D_in,self.D_in)\n",
    "#####        self.yNew = torch.empty(0, self.D_out,self.D_out)\n",
    "\n",
    "        numparams=sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print('parameters',numparams)\n",
    "\n",
    "        self.is_trained = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensuring the \"Spread\" of 2D Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good performance in our methodology has been attained by flattening solution data and taking the inner product of the data as a means to ensure  that data is sufficiently \"different\". This was originally implemented for 1D data, and has worked well (if not better) for flattended 2D data. Note that the code changes very little since we essentially only compare vectors of data :\n",
    "\n",
    "\n",
    "                blist.append(b)\n",
    "                reslist.append(res)\n",
    "                reslist_flat.append(np.reshape(res,(1,-1),order='C').squeeze(0))   \n",
    "                \n",
    "                # check orthogonality of 3 solutions that met training set critera\n",
    "                if   len(blist)==3 :\n",
    "                    resMat=np.asarray(reslist_flat)\n",
    "                    resMat_square=resMat**2\n",
    "                    row_sums = resMat_square.sum(axis=1,keepdims=True)\n",
    "                    resMat= resMat/np.sqrt(row_sums)\n",
    "                    InnerProd=np.dot(resMat,resMat.T)\n",
    "                    print('InnerProd',InnerProd)\n",
    "                    func.predictor.add(np.asarray(blist)[0], np.asarray(reslist)[0])\n",
    "                    cutoff=0.8\n",
    "                    \n",
    "                    # Picking out sufficiently orthogonal subset of 3 solutions gathered\n",
    "                    if np.abs(InnerProd[0,1]) and np.abs(InnerProd[0,2])<cutoff :\n",
    "                        if np.abs(InnerProd[1,2])<cutoff :\n",
    "                            func.predictor.add(np.asarray(blist)[1], np.asarray(reslist)[1])\n",
    "                            func.predictor.add(np.asarray(blist)[2], np.asarray(reslist)[2])\n",
    "                        elif np.abs(InnerProd[1,2])>=cutoff: \n",
    "                            func.predictor.add(np.asarray(blist)[1], np.asarray(reslist)[1])\n",
    "                    elif np.abs(InnerProd[0,1])<cutoff :\n",
    "                        func.predictor.add(np.asarray(blist)[1], np.asarray(reslist)[1])\n",
    "                    elif np.abs(InnerProd[0,2])<cutoff :\n",
    "                        func.predictor.add(np.asarray(blist)[2], np.asarray(reslist)[2])\n",
    "                    \n",
    "                    if func.predictor.counter>=retrain_freq:\n",
    "                        if func.debug:\n",
    "                            print(\"retraining\")\n",
    "                            print(func.predictor.counter)\n",
    "                            timeLoop=func.predictor.retrain_timed()\n",
    "                            trainTime=float(timeLoop[-1])\n",
    "                            blist=[]\n",
    "                            reslist=[]\n",
    "                            reslist_flat=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Architectures \n",
    "\n",
    "-Two simple neural network architectures have been found to be optimal in our experiments.  Note that optimal here means both a network that does not take too long to train (i.e not too deep or wide) and that quickly provides a speed-up to GMRES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2D CNN \n",
    "- Two  single channel 2D convolutional layers(varying kernel sizes) + 1 fully connected linear output layer\n",
    "    - The clear inspiration for this is the fact that the solution to the Poisson Equation can be expressed as a convolution of the RHS with the corresponding greens function. For a general linear operator $L$ we have\n",
    "    \n",
    "    $$Lu=f \\implies u=G*f$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1D CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $n^2$ channels 1D convolution(kernel size=n) + 1 Fully connected output layer\n",
    "    - The inspiration for this network can be understood when considering the underlying linear algebra problem for the discrete laplacian. In particular, we can think of the first layer as something not too different from the action of the matrix inverse on the RHS since we can express matrix multiplication of an $n\\times n$ with a vector of length $n$ as a 1D convolution with $n^2$ channels where the kernel for every channel is of length $n$.\n",
    "    \n",
    "    $$Ax=b \\implies x=A^{-1}b$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
