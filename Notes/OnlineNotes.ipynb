{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on terminology "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are subtle differences between online, continual, and incremental learning\n",
    "\n",
    "\n",
    "- The terminology is not particularly uniform or settled\n",
    "\n",
    "\n",
    "- The state of the art in these method (usually under the title of continual/continuous learning) seek to address the situation where the output of the neural networks may change as more data is collected. Since out output is constant for a given run (i.e we desire an output vector of a fixed size, we do not need to get too sophisticated. So, good \"search terms\" that fit our use are \"online\" and \"incremental\" learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note after the DAS/DSEG intern meeting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At some point during the meeting, I explained the underlining idea of our plans and desire to use online learning \n",
    "\n",
    "\n",
    "- They suggest we first directly try  traditional methods(i.e non-online, non-incremential, etc) on our \"goal problem\" first and show that solution data can actually speed up convergence before  trying to use more non-standard modalities.\n",
    " \n",
    " \n",
    "- Generally, they saw the path forward for us as either trying to emphasize depth in the project(jumping right to the \"goal\" problem), or breadth (exploring alternate approaches such as online methods first).\n",
    "\n",
    "\n",
    "\n",
    "- A personal note: I feel an intresting clash in methodologies here. Coming from a  physics/math background, we often start with \"toy\" problems to build intuition and build up from there. However, the inclination (at least in this group of data scientists) is to just jump into the deep end and throw whatever standard methods that may work, and then tweak things to really beat down the given problem.\n",
    "\n",
    "\n",
    "- A personal note: The \"in-situ\"\"realtime\" learning aspect of our problem was either not emphasized enough by me, or sort of overlooked by the suggestions given. It seems that the inclination was to immediately turn me back toward off-line learning. However, as we've seen, the ability to train quickly and effeciently is essential. I credit this misunderstanding to the fact that we're straying from the usual data science workflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Research Papers skimmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online Deep Learning: Learning Deep Neural Networks on the Fly: https://arxiv.org/pdf/1711.03705.pdf\n",
    "\n",
    "\n",
    "- Paper proposes  iteratively constructing a deeper network using a modified back propogation approach. Ultimately, the approach is too complicated for our relatively simple problem at hand\n",
    "\n",
    "\n",
    "- Has a decent and brief overview of online methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review papers skimmed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online Learning and Stochastic Approximations (2012): https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjl_8mw3NLpAhWMo54KHaOpCisQFjABegQIAxAB&url=https%3A%2F%2Fwww.cs.huji.ac.il%2F~shais%2Fpapers%2FOLsurvey.pdf&usg=AOvVaw0YA_MW2PUuxmC4dfA3Cdoc\n",
    "\n",
    "- A rigorous(theorem-proof structure) overview  of optimization methods for online learning. (Essentially a technical version of the online learning wikipedia page). Not clear how these methods mesh with deep learning as we need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online Learning and Stochastic Approximations (1998)\n",
    "\n",
    "https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjyqdrp2dLpAhXF6Z4KHaBJAfIQFjABegQIBBAB&url=https%3A%2F%2Fleon.bottou.org%2Fpublications%2Fpdf%2Fonline-1998.pdf&usg=AOvVaw3qK7TS52Rn59BxmanXnz1g\\n\n",
    "\n",
    "\n",
    "- A fairly dated overview of online optimization methods. Nontheless some useful formulations of online gradient descent  algorithms and applications are present here. In hindsight, it makes sense that ML researchers once really cared about this stuff. Storage was even more of an issue not too long ago...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Promising papers currently reading, but not yet read in desired detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continual Lifelong Learning with Neural Networks: A Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task-Free Continual Learning\n",
    "\n",
    "\n",
    "- proposes transforming a  continual learning problem to an online learning problems with data distributions gradually changing and without the notion of separate tasks (similar to our situation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Progress & Compress: A scalable framework for continual learning\n",
    "\n",
    "-  EWC seems promising for our problem as well as pretty easy to implement. There are some pytorch implementations out there as well. Here they talk about an online version which may be eben more useful"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
