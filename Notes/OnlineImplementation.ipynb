{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breif Overview of Our Implementation of an Online Training Approach for Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issues our implemtation hopes to address \n",
    "First, we note that it is well known that neural networks are a less than ideal tool for online machine learning problems. In particular, for the problems we are interested in, the following issues are particularly an issue for the problem we are looking at:\n",
    "\n",
    "- Scaling of training time of neural networks as dimensionality of data increases (as number of parameters in NN increases since our NN must take a \"n\" dimensional input and output an \"n\" dimensional output)\n",
    "\n",
    "\n",
    "- So called \"catastrophic forgetting\"/\"catastrophic inference\" is a generally encountered issue when training neural networks online. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breif description on how these issues are addressed in our implementation\n",
    "\n",
    "Primary approaches to reduce training times used are:\n",
    "\n",
    "- Optimize network size. For a given problem $Ax=b$, we know the solution $x$ provided $b$ can be obtained from the inverse operator(assuming it exists) $A^{-1}$ as $x=A^{-1}b$. Then, we know that our neural network that takes a $b$ and gives as a sufficiently \"close\" solution guess $x$ can be effectively represented as an operator with $O(n^2)$ weights (Note: we can think of matrix multiplication as a convolution operation  with stride 1, Kernel size $n$, and $n$ channels acting on an input of length $n$. In other words we take an inner product with $n$ unknown weights and the input vector of length $n$ an $n$ number of times). It was found through experimentation that using far more(or less) weights than something of order $n^2$ took longer to train. Thus, an optimal neural network to use that is not \"too big\" or \"too small\" is something that has $O(n^2)$ weights. We then set up something that more or less mimics the known \"true solution\" structure of matrix multilication with some neutal network \"depth\". Fortuitously, this involves using a large channel dimension, which is the dimension over which pytorch parallizes computations over.  \n",
    "\n",
    "\n",
    "        constructor: \n",
    "                device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "                self.Conv1   = torch.nn.Conv1d(1,int(H),D_in, stride=1, padding=0, dilation=1, groups=1, \n",
    "                    bias=False,padding_mode='zeros').to(device)\n",
    "                self.Conv2   = torch.nn.Conv1d(int(H),D_out,1, stride=1, padding=0, dilation=1, groups=1, \n",
    "                    bias=False,padding_mode='zeros').to(device)\n",
    "                self.relu   = torch.nn.LeakyReLU().to(device)\n",
    "                \n",
    "        forward:\n",
    "                Current_batchsize=int(x.shape[0])  # N in pytorch docs\n",
    "                device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "                x2=x.unsqueeze(1)  # Add channel dimension (C) to input \n",
    "                ConvOut1=self.relu(self.Conv1(x2.to(device)))\n",
    "                ConvOut2=self.Conv2(ConvOut1) \n",
    "                y_pred = ConvOut2.view(Current_batchsize, -1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Train the Neural network in an online way so that we do not have to retrain the network on the entire training set every time new data is added. \n",
    "    - We use an \"online batch\" stochastic gradient descent approach to incrementally train the neural network where we use new data gathered  along with a random sampling of  \"old data\" \n",
    "\n",
    "\n",
    "- \"Filter\" data added to the training set in some way so that we do not have data that is \"too redundant\" (i.e we try to maximize the \"value\" of data so training time is managable yet still valuable for speeding up computations)\n",
    "\n",
    "    - Filter approaches currently implented: \n",
    "    \n",
    "    Keep moving average of run time for GMRES computation up to a certain tolerance. Then, only add data when the current run time is larger than the moving average\n",
    "    \n",
    "    Add filtered data in small amounts to the training set and then retrain\n",
    "    \n",
    "    General philosophy: Only add data to training set that you are fitting poorly at the present \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excerpt from training pytorch loop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   def retrain_timed(self):\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        self.loss_val = list()  # clear loss val history\n",
    "        self.loss_val.append(10.0)\n",
    "\n",
    "        batch_size=64\n",
    "        numEpochs=2000\n",
    "        e1=1e-3\n",
    "        epoch=0\n",
    "        while self.loss_val[-1]> e1 and epoch<numEpochs:\n",
    "####            permutation = torch.randperm(self.x.size()[0])\n",
    "            for t in range(0,self.x.size()[0],batch_size):\n",
    "                \n",
    "####                indices = permutation[t:t+batch_size]\n",
    "\n",
    "####                batch_x, batch_y = self.x[indices],self.y[indices]\n",
    "\n",
    "####             batch_xMix=torch.cat((batch_x,self.xNew))\n",
    "####               batch_yMix=torch.cat((batch_y,self.yNew))\n",
    "\n",
    "                # Forward pass: Compute predicted y by passing x to the model\n",
    "                y_pred = self.model(batch_xMix.to(device))\n",
    "\n",
    "                # Compute and print loss\n",
    "                loss = self.criterion(y_pred, batch_yMix.to(device))\n",
    "                self.loss_val.append(loss.item())\n",
    "\n",
    "                # Zero gradients, perform a backward pass, and update the weights.\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch=epoch+1\n",
    "\n",
    "                #print diagnostic data\n",
    "                # print('loss:',loss.item())\n",
    "                # print('epoch:',epoch)\n",
    "        print('Final loss:',loss.item())\n",
    "        self.x=torch.cat((self.x,self.xNew))\n",
    "        self.y=torch.cat((self.y,self.yNew))\n",
    "####        self.xNew = torch.empty(0, self.D_in)\n",
    "####        self.yNew = torch.empty(0, self.D_out)\n",
    "        # numparams=sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        # print('parameters',numparams)\n",
    "        self.is_trained = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underlying idea of the code above is that for a given number of epochs, we sample a \"batch size\" amount of data and train with this data per Epoch. Note that at a given time, the model has already been trained with this data. The \"online\" twist to this approach is that we make sure to add the \"new\" data to these batches we will be training with. This way, we train the model that has never seen the \"new\" data with this \"new data\", while making sure it \"sees\" a spread out sampling of \"past data\". Note that in this present implementation, we only add up to 3 new data points at a time. \n",
    "\n",
    "The lines of code that implement these ideas are bolded above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code excerpt emphasizing data filters\n",
    "\n",
    "         res = target[-1]\n",
    "\n",
    "\n",
    "            # Check if we are in first e tolerance loop\n",
    "            if refine==False :\n",
    "                IterErr = resid(A, target, b)\n",
    "                IterTime=(toc-tic)\n",
    "                IterErr10=IterErr[10]\n",
    "                IterErrList.append(IterTime)\n",
    "                IterErrList10.append(IterErr10)  \n",
    "                if ProbCount<=Initial_set:\n",
    "                    func.predictor.add_init(b, res)\n",
    "                if ProbCount==Initial_set:\n",
    "                    func.predictor.add_init(b, res)\n",
    "                    timeLoop=func.predictor.retrain_timed()\n",
    "                    print('Initial Training')\n",
    "\n",
    "\n",
    "            # Compute moving averages used to filter data\n",
    "            if ProbCount>Initial_set:\n",
    "                IterTime_AVG=moving_average(np.asarray(IterErrList),ProbCount)\n",
    "                IterErr10_AVG=moving_average(np.asarray(IterErrList10),ProbCount)\n",
    "                print(IterErrList[-1],IterTime_AVG,IterErrList10[-1],IterErr10_AVG)\n",
    "\n",
    "\n",
    "            # Filter for data to be added to training set\n",
    "####            if (IterErrList[-1]>IterTime_AVG) and  refine==True and ProbCount>Initial_set   : \n",
    "                blist.append(b)\n",
    "                reslist.append(res)\n",
    "                \n",
    "                # check orthogonality of 3 solutions that met training set critera#\n",
    "####                if   len(blist)==3 :\n",
    "                    resMat=np.asarray(reslist)\n",
    "                    resMat_square=resMat**2\n",
    "                    row_sums = resMat_square.sum(axis=1,keepdims=True)\n",
    "                    resMat= resMat/np.sqrt(row_sums)\n",
    "                    InnerProd=np.dot(resMat,resMat.T)\n",
    "                    print('InnerProd',InnerProd)\n",
    "                    func.predictor.add(np.asarray(blist)[0], np.asarray(reslist)[0])\n",
    "                    cutoff=0.8\n",
    "                    \n",
    "####                    # Picking out sufficiently orthogonal subset of 3 solutions gathered\n",
    "####                    if InnerProd[0,1] and InnerProd[0,2]<cutoff :\n",
    "####                        if InnerProd[1,2]<cutoff :\n",
    "####                            func.predictor.add(np.asarray(blist)[1], np.asarray(reslist)[1])\n",
    "####                            func.predictor.add(np.asarray(blist)[2], np.asarray(reslist)[2])\n",
    "####                        elif InnerProd[1,2]>=cutoff: \n",
    "####                            func.predictor.add(np.asarray(blist)[1], np.asarray(reslist)[1])\n",
    "####                    elif InnerProd[0,1]<cutoff :\n",
    "####                        func.predictor.add(np.asarray(blist)[1], np.asarray(reslist)[1])\n",
    "####                    elif InnerProd[0,2]<cutoff :\n",
    "####                        func.predictor.add(np.asarray(blist)[2], np.asarray(reslist)[2])\n",
    "                    \n",
    "                    if func.predictor.counter>=retrain_freq:\n",
    "                        if func.debug:\n",
    "                            print(\"retraining\")\n",
    "                            print(func.predictor.counter)\n",
    "                            timeLoop=func.predictor.retrain_timed()\n",
    "                            trainTime=float(timeLoop[-1])\n",
    "                            blist=[]\n",
    "                            reslist=[]\n",
    "            return target,IterErrList,IterTime_AVG,trainTime,forwardTime,blist,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two main approaches were used to maximize the \"value\" of data in our training set. First, we only add data where our GMRES runtime to get a solution up to $e_1$ accuracy was longer than the current running average. In the code we use a moving average with a window of 25 samples. Of course, this window can be tweaked in a number of ways, but this seems to work well for the small experiments we have run so far. Of this data, we collect three at a time, and check the orthogonality of the three solutions computed. Of this set of 3, we add solutions that are sufficiently orthogonal to our training set. Ofcourse, we could write some code to check the orthogonality of a larger set of candidate solutions, or check the \"spread\" of solutions in some way. \n",
    "\n",
    "The lines of code that implement these ideas are bolded above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
