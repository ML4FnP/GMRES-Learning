{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "# Make logging folder named according to current time\n",
    "start_time_str = datetime.today().strftime('%d_%m_%Y__%H_%M_%S')\n",
    "log_dir = './logs/'f\"log_{start_time_str}\"\n",
    "os.makedirs(log_dir)\n",
    "\n",
    "# Initialize logger that adds to text file in current logging folder\n",
    "from logging_utils import *\n",
    "\n",
    "\n",
    "# Initialize logger that adds to text file in current logging folder\n",
    "from logging_utils import *\n",
    "init_logger(save_dir=log_dir)\n",
    "logger = get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import subprocess\n",
    "# Get the absolute path to your repository, \n",
    "# no matter where you are running this code from\n",
    "\n",
    "repo_path = os.getcwd() \n",
    "\n",
    "\n",
    "git_branch = subprocess.check_output(\n",
    "    [\"git\", \"-C\", repo_path, \"rev-parse\", \"--abbrev-ref\", \"HEAD\"]).strip().decode('UTF-8')\n",
    "\n",
    "git_commit_short_hash = subprocess.check_output(\n",
    "    [\"git\", \"-C\", repo_path, \"describe\", \"--always\"]).strip().decode('UTF-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dim=250\n",
    "x0=np.squeeze(np.zeros((dim,1)))\n",
    "x0Type='Zero Vector'\n",
    "\n",
    "FD_StencilMat=np.zeros((dim,dim))\n",
    "b = np.ones(dim-1)\n",
    "c = -2*np.ones(dim)\n",
    "np.fill_diagonal(FD_StencilMat[1:], b)\n",
    "np.fill_diagonal(FD_StencilMat[:,1:], b)\n",
    "np.fill_diagonal(FD_StencilMat, c)\n",
    "A=FD_StencilMat\n",
    "A=np.asmatrix(A)\n",
    "AType='1D Laplacian'\n",
    "\n",
    "retrain_freq=1\n",
    "e1 = 1e-3\n",
    "e2 = 1e-10\n",
    "nmax_iter = 50\n",
    "restart   = 64\n",
    "\n",
    "sigma=0.02\n",
    "DomainL=-5.0\n",
    "DomainR=5.0\n",
    "x=np.linspace(DomainL,DomainR,dim)\n",
    "\n",
    "\n",
    "\n",
    "n_steps =500\n",
    "InputDim=dim\n",
    "HiddenDim=dim\n",
    "HiddenDim2=dim\n",
    "OutputDim=dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-28 11:42:01 [INFO] <ipython-input-4-93f3238a1a0b> 2 - Benchmark 250 dim problem. Playing with SGD.\n"
     ]
    }
   ],
   "source": [
    "# Initial log message\n",
    "logger.info(\"Benchmark 250 dim problem. Playing with SGD.\")\n",
    "#  The source is a narrow gaussian that depends on the integer 'time' in a nonlinear way. The gaussian is on the interval [-5,5], and constrained to move on [-3,3]. \")\n",
    "\n",
    "\n",
    "\n",
    "# Set up parameter class\n",
    "class MyParamsClass():\n",
    "    def __init__(self, AType,dim,nmax_iter,restart,retrain_freq,e1,e2,x0Type,n_steps,InputDim,HiddenDim,HiddenDim2,OutputDim,sigma,DomainL,DomainR,git_branch,git_commit_short_hash):\n",
    "        self.AType = AType\n",
    "        self.dim = dim\n",
    "        self.nmax_iter = nmax_iter\n",
    "        self.restart = restart\n",
    "        self.n_steps =n_steps\n",
    "        self.retrain_freq=retrain_freq\n",
    "        self.e1=e1\n",
    "        self.e2=e2\n",
    "        self.x0Type=x0Type\n",
    "        self.InputDim=InputDim\n",
    "        self.HiddenDim=HiddenDim\n",
    "        self.HiddenDim2=HiddenDim2\n",
    "        self.OutputDim=OutputDim\n",
    "        self.sigma=sigma\n",
    "        self.DomainL=DomainL\n",
    "        self.DomainR=DomainR\n",
    "        self.git_branch=git_branch\n",
    "        self.git_commit_short_hash=git_commit_short_hash\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "params = MyParamsClass(AType,dim,nmax_iter,restart,retrain_freq,e1,e2,x0Type,n_steps,InputDim,HiddenDim,HiddenDim2,OutputDim,sigma,DomainL,DomainR,git_branch,git_commit_short_hash)\n",
    "sorted_params_dict = {k: params.__dict__[k] for k \n",
    "                      in sorted(params.__dict__.keys())}\n",
    "\n",
    "params_filepath = log_dir+'/params.json'\n",
    "json.dump(sorted_params_dict, open(params_filepath, 'w'), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src_dir import *\n",
    "\n",
    "# @nn_preconditioner(retrain_freq=retrain_freq, debug=True,InputDim=InputDim,HiddenDim=HiddenDim,HiddenDim2=HiddenDim2,OutputDim=OutputDim)\n",
    "# def MLGMRES(A, b, x0, e, nmax_iter,IterErr0_sum,IterErr0,ProbCount,Add,restart,debug):\n",
    "#     return GMRES(A, b, x0, e, nmax_iter,restart, debug)\n",
    "\n",
    "\n",
    "\n",
    "@timer\n",
    "@nn_preconditioner_timed(retrain_freq=retrain_freq, debug=True,InputDim=InputDim,HiddenDim=HiddenDim,HiddenDim2=HiddenDim2,OutputDim=OutputDim)\n",
    "def MLGMRES(A, b, x0, e, nmax_iter,Err_list,ProbCount,restart,debug,refine,blist,reslist,IterErrList10,GmresRunTime):\n",
    "    return GMRES(A, b, x0, e, nmax_iter,restart, debug)\n",
    "\n",
    "\n",
    "\n",
    "@timer\n",
    "def GMRES_timed(A, b, x0, e, nmax_iter,restart,debug):\n",
    "    return GMRES(A, b, x0, e, nmax_iter,restart, debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "loss: 2499442.0 0\n",
      "loss: 2495440.75 1\n",
      "loss: 2490003.5 2\n",
      "loss: 2482526.5 3\n",
      "loss: 2473072.5 4\n",
      "loss: 2461861.75 5\n",
      "loss: 2449035.0 6\n",
      "loss: 2434907.25 7\n",
      "loss: 2419519.5 8\n",
      "loss: 2403250.0 9\n",
      "loss: 2386291.75 10\n",
      "loss: 2368909.75 11\n",
      "loss: 2351219.0 12\n",
      "loss: 2333374.75 13\n",
      "loss: 2315447.25 14\n",
      "loss: 2297436.5 15\n",
      "loss: 2279407.0 16\n",
      "loss: 2261378.5 17\n",
      "loss: 2243367.5 18\n",
      "loss: 2225385.0 19\n",
      "loss: 2207440.75 20\n",
      "loss: 2189546.5 21\n",
      "loss: 2171714.5 22\n",
      "loss: 2153954.0 23\n",
      "loss: 2136272.5 24\n",
      "loss: 2118662.0 25\n",
      "loss: 2101142.5 26\n",
      "loss: 2083718.0 27\n",
      "loss: 2066392.5 28\n",
      "loss: 2049167.875 29\n",
      "loss: 2032033.625 30\n",
      "loss: 2015008.75 31\n",
      "loss: 1998094.0 32\n",
      "loss: 1981290.375 33\n",
      "loss: 1964599.25 34\n",
      "loss: 1948021.875 35\n",
      "loss: 1931559.25 36\n",
      "loss: 1915212.0 37\n",
      "loss: 1898980.875 38\n",
      "loss: 1882866.5 39\n",
      "loss: 1866869.0 40\n",
      "loss: 1850989.125 41\n",
      "loss: 1835226.5 42\n",
      "loss: 1819581.375 43\n",
      "loss: 1804054.0 44\n",
      "loss: 1788644.25 45\n",
      "loss: 1773351.75 46\n",
      "loss: 1758176.625 47\n",
      "loss: 1743118.5 48\n",
      "loss: 1728177.0 49\n",
      "loss: 1713352.125 50\n",
      "loss: 1698643.25 51\n",
      "loss: 1684050.125 52\n",
      "loss: 1669572.25 53\n",
      "loss: 1655209.25 54\n",
      "loss: 1640960.5 55\n",
      "loss: 1626825.75 56\n",
      "loss: 1612804.375 57\n",
      "loss: 1598895.75 58\n",
      "loss: 1585099.5 59\n",
      "loss: 1571414.75 60\n",
      "loss: 1557841.5 61\n",
      "loss: 1544378.5 62\n",
      "loss: 1531025.5 63\n",
      "loss: 1517781.625 64\n",
      "loss: 1504646.625 65\n",
      "loss: 1491619.75 66\n",
      "loss: 1478700.25 67\n",
      "loss: 1465887.25 68\n",
      "loss: 1453180.75 69\n",
      "loss: 1440579.375 70\n",
      "loss: 1428083.0 71\n",
      "loss: 1415690.625 72\n",
      "loss: 1403401.875 73\n",
      "loss: 1391215.875 74\n",
      "loss: 1379131.75 75\n",
      "loss: 1367149.0 76\n",
      "loss: 1355267.125 77\n",
      "loss: 1343485.25 78\n",
      "loss: 1331802.875 79\n",
      "loss: 1320219.125 80\n",
      "loss: 1308733.5 81\n",
      "loss: 1297345.0 82\n",
      "loss: 1286053.25 83\n",
      "loss: 1274857.5 84\n",
      "loss: 1263757.0 85\n",
      "loss: 1252751.0 86\n",
      "loss: 1241839.0 87\n",
      "loss: 1231020.375 88\n",
      "loss: 1220294.0 89\n",
      "loss: 1209659.75 90\n",
      "loss: 1199116.625 91\n",
      "loss: 1188664.125 92\n",
      "loss: 1178301.375 93\n",
      "loss: 1168028.0 94\n",
      "loss: 1157843.0 95\n",
      "loss: 1147746.0 96\n",
      "loss: 1137736.125 97\n",
      "loss: 1127812.875 98\n",
      "loss: 1117975.5 99\n",
      "loss: 1108223.375 100\n",
      "loss: 1098555.875 101\n",
      "loss: 1088972.25 102\n",
      "loss: 1079472.125 103\n",
      "loss: 1070054.625 104\n",
      "loss: 1060719.0 105\n",
      "loss: 1051465.0 106\n",
      "loss: 1042291.5625 107\n",
      "loss: 1033198.375 108\n",
      "loss: 1024184.625 109\n",
      "loss: 1015249.875 110\n",
      "loss: 1006393.5 111\n",
      "loss: 997614.625 112\n",
      "loss: 988912.8125 113\n",
      "loss: 980287.375 114\n",
      "loss: 971737.875 115\n",
      "loss: 963263.5625 116\n",
      "loss: 954864.0 117\n",
      "loss: 946538.375 118\n",
      "loss: 938286.25 119\n",
      "loss: 930107.125 120\n",
      "loss: 922000.125 121\n",
      "loss: 913964.875 122\n",
      "loss: 906000.8125 123\n",
      "loss: 898107.25 124\n",
      "loss: 890283.625 125\n",
      "loss: 882529.5625 126\n",
      "loss: 874844.375 127\n",
      "loss: 867227.4375 128\n",
      "loss: 859678.25 129\n",
      "loss: 852196.25 130\n",
      "loss: 844780.9375 131\n",
      "loss: 837431.6875 132\n",
      "loss: 830148.0 133\n",
      "loss: 822929.3125 134\n",
      "loss: 815775.1875 135\n",
      "loss: 808685.0625 136\n",
      "loss: 801658.3125 137\n",
      "loss: 794694.5625 138\n",
      "loss: 787793.125 139\n",
      "loss: 780953.5625 140\n",
      "loss: 774175.5 141\n",
      "loss: 767458.1875 142\n",
      "loss: 760801.25 143\n",
      "loss: 754204.25 144\n",
      "loss: 747666.5625 145\n",
      "loss: 741187.75 146\n",
      "loss: 734767.25 147\n",
      "loss: 728404.625 148\n",
      "loss: 722099.5625 149\n",
      "loss: 715851.25 150\n",
      "loss: 709659.5 151\n",
      "loss: 703523.625 152\n",
      "loss: 697443.25 153\n",
      "loss: 691418.0 154\n",
      "loss: 685447.25 155\n",
      "loss: 679530.625 156\n",
      "loss: 673667.625 157\n",
      "loss: 667857.875 158\n",
      "loss: 662100.8125 159\n",
      "loss: 656396.125 160\n",
      "loss: 650743.1875 161\n",
      "loss: 645141.6875 162\n",
      "loss: 639591.25 163\n",
      "loss: 634091.25 164\n",
      "loss: 628641.375 165\n",
      "loss: 623241.1875 166\n",
      "loss: 617890.25 167\n",
      "loss: 612588.1875 168\n",
      "loss: 607334.5 169\n",
      "loss: 602128.875 170\n",
      "loss: 596970.75 171\n",
      "loss: 591859.8125 172\n",
      "loss: 586795.75 173\n",
      "loss: 581778.0 174\n",
      "loss: 576806.1875 175\n",
      "loss: 571879.9375 176\n",
      "loss: 566998.875 177\n",
      "loss: 562162.625 178\n",
      "loss: 557370.75 179\n",
      "loss: 552622.9375 180\n",
      "loss: 547918.75 181\n",
      "loss: 543257.75 182\n",
      "loss: 538639.625 183\n",
      "loss: 534064.0 184\n",
      "loss: 529530.5 185\n",
      "loss: 525038.8125 186\n",
      "loss: 520588.4375 187\n",
      "loss: 516179.1875 188\n",
      "loss: 511810.59375 189\n",
      "loss: 507482.3125 190\n",
      "loss: 503193.9375 191\n",
      "loss: 498945.21875 192\n",
      "loss: 494735.8125 193\n",
      "loss: 490565.3125 194\n",
      "loss: 486433.34375 195\n",
      "loss: 482339.625 196\n",
      "loss: 478283.8125 197\n",
      "loss: 474265.5625 198\n",
      "loss: 470284.5625 199\n",
      "loss: 466340.4375 200\n",
      "loss: 462432.875 201\n",
      "loss: 458561.59375 202\n",
      "loss: 454726.21875 203\n",
      "loss: 450926.5 204\n",
      "loss: 447162.03125 205\n",
      "loss: 443432.5625 206\n",
      "loss: 439737.8125 207\n",
      "loss: 436077.34375 208\n",
      "loss: 432451.0 209\n",
      "loss: 428858.375 210\n",
      "loss: 425299.1875 211\n",
      "loss: 421773.125 212\n",
      "loss: 418279.9375 213\n",
      "loss: 414819.3125 214\n",
      "loss: 411391.0 215\n",
      "loss: 407994.625 216\n",
      "loss: 404629.9375 217\n",
      "loss: 401296.6875 218\n",
      "loss: 397994.5 219\n",
      "loss: 394723.1875 220\n",
      "loss: 391482.4375 221\n",
      "loss: 388272.0 222\n",
      "loss: 385091.5625 223\n",
      "loss: 381940.9375 224\n",
      "loss: 378819.75 225\n",
      "loss: 375727.71875 226\n",
      "loss: 372664.65625 227\n",
      "loss: 369630.21875 228\n",
      "loss: 366624.1875 229\n",
      "loss: 363646.40625 230\n",
      "loss: 360696.5 231\n",
      "loss: 357774.25 232\n",
      "loss: 354879.4375 233\n",
      "loss: 352011.6875 234\n",
      "loss: 349170.9375 235\n",
      "loss: 346356.8125 236\n",
      "loss: 343569.125 237\n",
      "loss: 340807.625 238\n",
      "loss: 338072.0625 239\n",
      "loss: 335362.15625 240\n",
      "loss: 332677.75 241\n",
      "loss: 330018.59375 242\n",
      "loss: 327384.40625 243\n",
      "loss: 324774.96875 244\n",
      "loss: 322190.125 245\n",
      "loss: 319629.59375 246\n",
      "loss: 317093.125 247\n",
      "loss: 314580.5625 248\n",
      "loss: 312091.6875 249\n",
      "loss: 309626.25 250\n",
      "loss: 307183.96875 251\n",
      "loss: 304764.75 252\n",
      "loss: 302368.3125 253\n",
      "loss: 299994.5 254\n",
      "loss: 297643.0 255\n",
      "loss: 295313.71875 256\n",
      "loss: 293006.375 257\n",
      "loss: 290720.78125 258\n",
      "loss: 288456.6875 259\n",
      "loss: 286214.03125 260\n",
      "loss: 283992.53125 261\n",
      "loss: 281791.96875 262\n",
      "loss: 279612.21875 263\n",
      "loss: 277453.03125 264\n",
      "loss: 275314.21875 265\n",
      "loss: 273195.625 266\n",
      "loss: 271097.09375 267\n",
      "loss: 269018.34375 268\n",
      "loss: 266959.25 269\n",
      "loss: 264919.5625 270\n",
      "loss: 262899.1875 271\n",
      "loss: 260897.921875 272\n",
      "loss: 258915.546875 273\n",
      "loss: 256951.90625 274\n",
      "loss: 255006.84375 275\n",
      "loss: 253080.1875 276\n",
      "loss: 251171.734375 277\n",
      "loss: 249281.375 278\n",
      "loss: 247408.84375 279\n",
      "loss: 245554.015625 280\n",
      "loss: 243716.765625 281\n",
      "loss: 241896.875 282\n",
      "loss: 240094.25 283\n",
      "loss: 238308.65625 284\n",
      "loss: 236539.96875 285\n",
      "loss: 234787.96875 286\n",
      "loss: 233052.65625 287\n",
      "loss: 231333.6875 288\n",
      "loss: 229631.03125 289\n",
      "loss: 227944.5 290\n",
      "loss: 226273.90625 291\n",
      "loss: 224619.125 292\n",
      "loss: 222980.03125 293\n",
      "loss: 221356.46875 294\n",
      "loss: 219748.28125 295\n",
      "loss: 218155.3125 296\n",
      "loss: 216577.4375 297\n",
      "loss: 215014.5 298\n",
      "loss: 213466.375 299\n",
      "loss: 211932.890625 300\n",
      "loss: 210413.9375 301\n",
      "loss: 208909.375 302\n",
      "loss: 207419.046875 303\n",
      "loss: 205942.84375 304\n",
      "loss: 204480.65625 305\n",
      "loss: 203032.265625 306\n",
      "loss: 201597.59375 307\n",
      "loss: 200176.53125 308\n",
      "loss: 198768.9375 309\n",
      "loss: 197374.640625 310\n",
      "loss: 195993.59375 311\n",
      "loss: 194625.59375 312\n",
      "loss: 193270.5625 313\n",
      "loss: 191928.375 314\n",
      "loss: 190598.859375 315\n",
      "loss: 189281.9375 316\n",
      "loss: 187977.515625 317\n",
      "loss: 186685.4375 318\n",
      "loss: 185405.59375 319\n",
      "loss: 184137.84375 320\n",
      "loss: 182882.09375 321\n",
      "loss: 181638.28125 322\n",
      "loss: 180406.1875 323\n",
      "loss: 179185.78125 324\n",
      "loss: 177976.90625 325\n",
      "loss: 176779.5 326\n",
      "loss: 175593.40625 327\n",
      "loss: 174418.53125 328\n",
      "loss: 173254.8125 329\n",
      "loss: 172102.046875 330\n",
      "loss: 170960.21875 331\n",
      "loss: 169829.1875 332\n",
      "loss: 168708.84375 333\n",
      "loss: 167599.09375 334\n",
      "loss: 166499.859375 335\n",
      "loss: 165411.03125 336\n",
      "loss: 164332.453125 337\n",
      "loss: 163264.09375 338\n",
      "loss: 162205.84375 339\n",
      "loss: 161157.5625 340\n",
      "loss: 160119.1875 341\n",
      "loss: 159090.640625 342\n",
      "loss: 158071.8125 343\n",
      "loss: 157062.640625 344\n",
      "loss: 156062.953125 345\n",
      "loss: 155072.71875 346\n",
      "loss: 154091.828125 347\n",
      "loss: 153120.21875 348\n",
      "loss: 152157.75 349\n",
      "loss: 151204.375 350\n",
      "loss: 150260.015625 351\n",
      "loss: 149324.53125 352\n",
      "loss: 148397.9375 353\n",
      "loss: 147480.03125 354\n",
      "loss: 146570.8125 355\n",
      "loss: 145670.125 356\n",
      "loss: 144777.9375 357\n",
      "loss: 143894.15625 358\n",
      "loss: 143018.75 359\n",
      "loss: 142151.5625 360\n",
      "loss: 141292.53125 361\n",
      "loss: 140441.609375 362\n",
      "loss: 139598.703125 363\n",
      "loss: 138763.71875 364\n",
      "loss: 137936.59375 365\n",
      "loss: 137117.28125 366\n",
      "loss: 136305.65625 367\n",
      "loss: 135501.671875 368\n",
      "loss: 134705.25 369\n",
      "loss: 133916.359375 370\n",
      "loss: 133134.8125 371\n",
      "loss: 132360.65625 372\n",
      "loss: 131593.765625 373\n",
      "loss: 130834.0625 374\n",
      "loss: 130081.5 375\n",
      "loss: 129336.0 376\n",
      "loss: 128597.484375 377\n",
      "loss: 127865.953125 378\n",
      "loss: 127141.2578125 379\n",
      "loss: 126423.375 380\n",
      "loss: 125712.203125 381\n",
      "loss: 125007.703125 382\n",
      "loss: 124309.8203125 383\n",
      "loss: 123618.5 384\n",
      "loss: 122933.65625 385\n",
      "loss: 122255.203125 386\n",
      "loss: 121583.1171875 387\n",
      "loss: 120917.3125 388\n",
      "loss: 120257.765625 389\n",
      "loss: 119604.375 390\n",
      "loss: 118957.09375 391\n",
      "loss: 118315.875 392\n",
      "loss: 117680.65625 393\n",
      "loss: 117051.34375 394\n",
      "loss: 116427.953125 395\n",
      "loss: 115810.3515625 396\n",
      "loss: 115198.546875 397\n",
      "loss: 114592.453125 398\n",
      "loss: 113992.0234375 399\n",
      "loss: 113397.171875 400\n",
      "loss: 112807.875 401\n",
      "loss: 112224.09375 402\n",
      "loss: 111645.71875 403\n",
      "loss: 111072.7890625 404\n",
      "loss: 110505.171875 405\n",
      "loss: 109942.84375 406\n",
      "loss: 109385.765625 407\n",
      "loss: 108833.859375 408\n",
      "loss: 108287.078125 409\n",
      "loss: 107745.421875 410\n",
      "loss: 107208.78125 411\n",
      "loss: 106677.140625 412\n",
      "loss: 106150.453125 413\n",
      "loss: 105628.625 414\n",
      "loss: 105111.6953125 415\n",
      "loss: 104599.53125 416\n",
      "loss: 104092.140625 417\n",
      "loss: 103589.4296875 418\n",
      "loss: 103091.4140625 419\n",
      "loss: 102597.9921875 420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 102109.1484375 421\n",
      "loss: 101624.859375 422\n",
      "loss: 101145.015625 423\n",
      "loss: 100669.640625 424\n",
      "loss: 100198.65625 425\n",
      "loss: 99732.046875 426\n",
      "loss: 99269.734375 427\n",
      "loss: 98811.6953125 428\n",
      "loss: 98357.921875 429\n",
      "loss: 97908.34375 430\n",
      "loss: 97462.890625 431\n",
      "loss: 97021.5625 432\n",
      "loss: 96584.3203125 433\n",
      "loss: 96151.1015625 434\n",
      "loss: 95721.890625 435\n",
      "loss: 95296.6171875 436\n",
      "loss: 94875.2890625 437\n",
      "loss: 94457.828125 438\n",
      "loss: 94044.234375 439\n",
      "loss: 93634.421875 440\n",
      "loss: 93228.3984375 441\n",
      "loss: 92826.109375 442\n",
      "loss: 92427.515625 443\n",
      "loss: 92032.59375 444\n",
      "loss: 91641.3125 445\n",
      "loss: 91253.6015625 446\n",
      "loss: 90869.453125 447\n",
      "loss: 90488.84375 448\n",
      "loss: 90111.71875 449\n",
      "loss: 89738.0703125 450\n",
      "loss: 89367.84375 451\n",
      "loss: 89001.0 452\n",
      "loss: 88637.53125 453\n",
      "loss: 88277.359375 454\n",
      "loss: 87920.5 455\n",
      "loss: 87566.9375 456\n",
      "loss: 87216.578125 457\n",
      "loss: 86869.4375 458\n",
      "loss: 86525.4375 459\n",
      "loss: 86184.625 460\n",
      "loss: 85846.890625 461\n",
      "loss: 85512.2578125 462\n",
      "loss: 85180.6796875 463\n",
      "loss: 84852.1328125 464\n",
      "loss: 84526.546875 465\n",
      "loss: 84203.96875 466\n",
      "loss: 83884.296875 467\n",
      "loss: 83567.5625 468\n",
      "loss: 83253.6953125 469\n",
      "loss: 82942.6875 470\n",
      "loss: 82634.5 471\n",
      "loss: 82329.1171875 472\n",
      "loss: 82026.53125 473\n",
      "loss: 81726.6875 474\n",
      "loss: 81429.546875 475\n",
      "loss: 81135.0859375 476\n",
      "loss: 80843.34375 477\n",
      "loss: 80554.234375 478\n",
      "loss: 80267.71875 479\n",
      "loss: 79983.8125 480\n",
      "loss: 79702.484375 481\n",
      "loss: 79423.703125 482\n",
      "loss: 79147.421875 483\n",
      "loss: 78873.65625 484\n",
      "loss: 78602.3671875 485\n",
      "loss: 78333.5390625 486\n",
      "loss: 78067.1171875 487\n",
      "loss: 77803.1015625 488\n",
      "loss: 77541.46875 489\n",
      "loss: 77282.203125 490\n",
      "loss: 77025.265625 491\n",
      "loss: 76770.65625 492\n",
      "loss: 76518.34375 493\n",
      "loss: 76268.28125 494\n",
      "loss: 76020.4765625 495\n",
      "loss: 75774.90625 496\n",
      "loss: 75531.546875 497\n",
      "loss: 75290.3515625 498\n",
      "loss: 75051.3359375 499\n",
      "loss: 74814.46875 500\n",
      "loss: 74579.7109375 501\n",
      "loss: 74347.0859375 502\n",
      "loss: 74116.53125 503\n",
      "loss: 73888.046875 504\n",
      "loss: 73661.6015625 505\n",
      "loss: 73437.171875 506\n",
      "loss: 73214.78125 507\n",
      "loss: 72994.359375 508\n",
      "loss: 72775.921875 509\n",
      "loss: 72559.4140625 510\n",
      "loss: 72344.8515625 511\n",
      "loss: 72132.1875 512\n",
      "loss: 71921.453125 513\n",
      "loss: 71712.5703125 514\n",
      "loss: 71505.5625 515\n",
      "loss: 71300.375 516\n",
      "loss: 71097.0546875 517\n",
      "loss: 70895.515625 518\n",
      "loss: 70695.7578125 519\n",
      "loss: 70497.8046875 520\n",
      "loss: 70301.59375 521\n",
      "loss: 70107.1171875 522\n",
      "loss: 69914.390625 523\n",
      "loss: 69723.375 524\n",
      "loss: 69534.03125 525\n",
      "loss: 69346.3828125 526\n",
      "loss: 69160.390625 527\n",
      "loss: 68976.03125 528\n",
      "loss: 68793.3125 529\n",
      "loss: 68612.2265625 530\n",
      "loss: 68432.7265625 531\n",
      "loss: 68254.8203125 532\n",
      "loss: 68078.46875 533\n",
      "loss: 67903.6875 534\n",
      "loss: 67730.4375 535\n",
      "loss: 67558.734375 536\n",
      "loss: 67388.5390625 537\n",
      "loss: 67219.828125 538\n",
      "loss: 67052.609375 539\n",
      "loss: 66886.875 540\n",
      "loss: 66722.59375 541\n",
      "loss: 66559.7265625 542\n",
      "loss: 66398.328125 543\n",
      "loss: 66238.3203125 544\n",
      "loss: 66079.7421875 545\n",
      "loss: 65922.53125 546\n",
      "loss: 65766.71875 547\n",
      "loss: 65612.265625 548\n",
      "loss: 65459.1640625 549\n",
      "loss: 65307.38671875 550\n",
      "loss: 65156.96484375 551\n",
      "loss: 65007.8515625 552\n",
      "loss: 64860.03125 553\n",
      "loss: 64713.5078125 554\n",
      "loss: 64568.2578125 555\n",
      "loss: 64424.2890625 556\n",
      "loss: 64281.56640625 557\n",
      "loss: 64140.08203125 558\n",
      "loss: 63999.84375 559\n",
      "loss: 63860.8125 560\n",
      "loss: 63723.00390625 561\n",
      "loss: 63586.39453125 562\n",
      "loss: 63450.9609375 563\n",
      "loss: 63316.70703125 564\n",
      "loss: 63183.6328125 565\n",
      "loss: 63051.69921875 566\n",
      "loss: 62920.921875 567\n",
      "loss: 62791.265625 568\n",
      "loss: 62662.7421875 569\n",
      "loss: 62535.328125 570\n",
      "loss: 62409.0078125 571\n",
      "loss: 62283.7890625 572\n",
      "loss: 62159.6640625 573\n",
      "loss: 62036.59375 574\n",
      "loss: 61914.59375 575\n",
      "loss: 61793.640625 576\n",
      "loss: 61673.7421875 577\n",
      "loss: 61554.8671875 578\n",
      "loss: 61437.0234375 579\n",
      "loss: 61320.1875 580\n",
      "loss: 61204.37109375 581\n",
      "loss: 61089.546875 582\n",
      "loss: 60975.69140625 583\n",
      "loss: 60862.84765625 584\n",
      "loss: 60750.9453125 585\n",
      "loss: 60640.0 586\n",
      "loss: 60530.02734375 587\n",
      "loss: 60421.0 588\n",
      "loss: 60312.8828125 589\n",
      "loss: 60205.71875 590\n",
      "loss: 60099.45703125 591\n",
      "loss: 59994.1015625 592\n",
      "loss: 59889.65625 593\n",
      "loss: 59786.11328125 594\n",
      "loss: 59683.4453125 595\n",
      "loss: 59581.640625 596\n",
      "loss: 59480.71484375 597\n",
      "loss: 59380.65625 598\n",
      "loss: 59281.4296875 599\n",
      "loss: 59183.078125 600\n",
      "loss: 59085.53125 601\n",
      "loss: 58988.83203125 602\n",
      "loss: 58892.953125 603\n",
      "loss: 58797.890625 604\n",
      "loss: 58703.6328125 605\n",
      "loss: 58610.17578125 606\n",
      "loss: 58517.51953125 607\n",
      "loss: 58425.6328125 608\n",
      "loss: 58334.53125 609\n",
      "loss: 58244.203125 610\n",
      "loss: 58154.64453125 611\n",
      "loss: 58065.8515625 612\n",
      "loss: 57977.78125 613\n",
      "loss: 57890.4609375 614\n",
      "loss: 57803.88671875 615\n",
      "loss: 57718.0546875 616\n",
      "loss: 57632.9296875 617\n",
      "loss: 57548.53125 618\n",
      "loss: 57464.84375 619\n",
      "loss: 57381.85546875 620\n",
      "loss: 57299.56640625 621\n",
      "loss: 57217.9765625 622\n",
      "loss: 57137.05078125 623\n",
      "loss: 57056.828125 624\n",
      "loss: 56977.265625 625\n",
      "loss: 56898.3828125 626\n",
      "loss: 56820.15234375 627\n",
      "loss: 56742.5859375 628\n",
      "loss: 56665.671875 629\n",
      "loss: 56589.390625 630\n",
      "loss: 56513.7421875 631\n",
      "loss: 56438.7421875 632\n",
      "loss: 56364.359375 633\n",
      "loss: 56290.6015625 634\n",
      "loss: 56217.46875 635\n",
      "loss: 56144.9296875 636\n",
      "loss: 56072.9921875 637\n",
      "loss: 56001.671875 638\n",
      "loss: 55930.9296875 639\n",
      "loss: 55860.7890625 640\n",
      "loss: 55791.22265625 641\n",
      "loss: 55722.23046875 642\n",
      "loss: 55653.8125 643\n",
      "loss: 55585.98046875 644\n",
      "loss: 55518.69921875 645\n",
      "loss: 55451.9765625 646\n",
      "loss: 55385.80078125 647\n",
      "loss: 55320.1875 648\n",
      "loss: 55255.10546875 649\n",
      "loss: 55190.55859375 650\n",
      "loss: 55126.55078125 651\n",
      "loss: 55063.07421875 652\n",
      "loss: 55000.1328125 653\n",
      "loss: 54937.703125 654\n",
      "loss: 54875.7734375 655\n",
      "loss: 54814.3671875 656\n",
      "loss: 54753.45703125 657\n",
      "loss: 54693.0625 658\n",
      "loss: 54633.15234375 659\n",
      "loss: 54573.734375 660\n",
      "loss: 54514.8203125 661\n",
      "loss: 54456.3671875 662\n",
      "loss: 54398.41796875 663\n",
      "loss: 54340.921875 664\n",
      "loss: 54283.89453125 665\n",
      "loss: 54227.3515625 666\n",
      "loss: 54171.2578125 667\n",
      "loss: 54115.6328125 668\n",
      "loss: 54060.4609375 669\n",
      "loss: 54005.73828125 670\n",
      "loss: 53951.46484375 671\n",
      "loss: 53897.625 672\n",
      "loss: 53844.234375 673\n",
      "loss: 53791.28125 674\n",
      "loss: 53738.75 675\n",
      "loss: 53686.640625 676\n",
      "loss: 53634.97265625 677\n",
      "loss: 53583.71875 678\n",
      "loss: 53532.88671875 679\n",
      "loss: 53482.44140625 680\n",
      "loss: 53432.4375 681\n",
      "loss: 53382.8125 682\n",
      "loss: 53333.625 683\n",
      "loss: 53284.79296875 684\n",
      "loss: 53236.3828125 685\n",
      "loss: 53188.3515625 686\n",
      "loss: 53140.70703125 687\n",
      "loss: 53093.4765625 688\n",
      "loss: 53046.6015625 689\n",
      "loss: 53000.09375 690\n",
      "loss: 52953.98046875 691\n",
      "loss: 52908.2578125 692\n",
      "loss: 52862.875 693\n",
      "loss: 52817.859375 694\n",
      "loss: 52773.19921875 695\n",
      "loss: 52728.91015625 696\n",
      "loss: 52684.9765625 697\n",
      "loss: 52641.39453125 698\n",
      "loss: 52598.16015625 699\n",
      "loss: 52555.2890625 700\n",
      "loss: 52512.7421875 701\n",
      "loss: 52470.51953125 702\n",
      "loss: 52428.67578125 703\n",
      "loss: 52387.1328125 704\n",
      "loss: 52345.93359375 705\n",
      "loss: 52305.0703125 706\n",
      "loss: 52264.515625 707\n",
      "loss: 52224.2890625 708\n",
      "loss: 52184.39453125 709\n",
      "loss: 52144.82421875 710\n",
      "loss: 52105.55078125 711\n",
      "loss: 52066.6015625 712\n",
      "loss: 52027.953125 713\n",
      "loss: 51989.59765625 714\n",
      "loss: 51951.578125 715\n",
      "loss: 51913.84765625 716\n",
      "loss: 51876.40625 717\n",
      "loss: 51839.2890625 718\n",
      "loss: 51802.43359375 719\n",
      "loss: 51765.875 720\n",
      "loss: 51729.6171875 721\n",
      "loss: 51693.6328125 722\n",
      "loss: 51657.9453125 723\n",
      "loss: 51622.54296875 724\n",
      "loss: 51587.40625 725\n",
      "loss: 51552.5546875 726\n",
      "loss: 51517.984375 727\n",
      "loss: 51483.65625 728\n",
      "loss: 51449.61328125 729\n",
      "loss: 51415.84375 730\n",
      "loss: 51382.328125 731\n",
      "loss: 51349.09375 732\n",
      "loss: 51316.1171875 733\n",
      "loss: 51283.375 734\n",
      "loss: 51250.90625 735\n",
      "loss: 51218.69921875 736\n",
      "loss: 51186.7421875 737\n",
      "loss: 51155.015625 738\n",
      "loss: 51123.55078125 739\n",
      "loss: 51092.3125 740\n",
      "loss: 51061.3359375 741\n",
      "loss: 51030.625 742\n",
      "loss: 51000.11328125 743\n",
      "loss: 50969.8515625 744\n",
      "loss: 50939.82421875 745\n",
      "loss: 50910.0390625 746\n",
      "loss: 50880.47265625 747\n",
      "loss: 50851.1484375 748\n",
      "loss: 50822.04296875 749\n",
      "loss: 50793.15625 750\n",
      "loss: 50764.5078125 751\n",
      "loss: 50736.078125 752\n",
      "loss: 50707.86328125 753\n",
      "loss: 50679.86328125 754\n",
      "loss: 50652.078125 755\n",
      "loss: 50624.515625 756\n",
      "loss: 50597.1640625 757\n",
      "loss: 50570.0234375 758\n",
      "loss: 50543.08984375 759\n",
      "loss: 50516.359375 760\n",
      "loss: 50489.84765625 761\n",
      "loss: 50463.51953125 762\n",
      "loss: 50437.3984375 763\n",
      "loss: 50411.484375 764\n",
      "loss: 50385.77734375 765\n",
      "loss: 50360.2421875 766\n",
      "loss: 50334.92578125 767\n",
      "loss: 50309.7890625 768\n",
      "loss: 50284.85546875 769\n",
      "loss: 50260.1015625 770\n",
      "loss: 50235.54296875 771\n",
      "loss: 50211.171875 772\n",
      "loss: 50186.98046875 773\n",
      "loss: 50162.9765625 774\n",
      "loss: 50139.1484375 775\n",
      "loss: 50115.5078125 776\n",
      "loss: 50092.05078125 777\n",
      "loss: 50068.7734375 778\n",
      "loss: 50045.65625 779\n",
      "loss: 50022.73046875 780\n",
      "loss: 49999.9765625 781\n",
      "loss: 49977.37109375 782\n",
      "loss: 49954.95703125 783\n",
      "loss: 49932.6953125 784\n",
      "loss: 49910.62109375 785\n",
      "loss: 49888.7109375 786\n",
      "loss: 49866.96875 787\n",
      "loss: 49845.38671875 788\n",
      "loss: 49823.9453125 789\n",
      "loss: 49802.68359375 790\n",
      "loss: 49781.58203125 791\n",
      "loss: 49760.640625 792\n",
      "loss: 49739.85546875 793\n",
      "loss: 49719.2109375 794\n",
      "loss: 49698.734375 795\n",
      "loss: 49678.421875 796\n",
      "loss: 49658.25 797\n",
      "loss: 49638.2265625 798\n",
      "loss: 49618.34375 799\n",
      "loss: 49598.609375 800\n",
      "loss: 49579.046875 801\n",
      "loss: 49559.6015625 802\n",
      "loss: 49540.3125 803\n",
      "loss: 49521.1640625 804\n",
      "loss: 49502.1640625 805\n",
      "loss: 49483.30859375 806\n",
      "loss: 49464.5703125 807\n",
      "loss: 49445.9921875 808\n",
      "loss: 49427.54296875 809\n",
      "loss: 49409.234375 810\n",
      "loss: 49391.0703125 811\n",
      "loss: 49373.01953125 812\n",
      "loss: 49355.125 813\n",
      "loss: 49337.328125 814\n",
      "loss: 49319.6953125 815\n",
      "loss: 49302.16796875 816\n",
      "loss: 49284.78515625 817\n",
      "loss: 49267.53125 818\n",
      "loss: 49250.390625 819\n",
      "loss: 49233.375 820\n",
      "loss: 49216.5 821\n",
      "loss: 49199.734375 822\n",
      "loss: 49183.1015625 823\n",
      "loss: 49166.578125 824\n",
      "loss: 49150.171875 825\n",
      "loss: 49133.890625 826\n",
      "loss: 49117.74609375 827\n",
      "loss: 49101.703125 828\n",
      "loss: 49085.7890625 829\n",
      "loss: 49069.96875 830\n",
      "loss: 49054.2734375 831\n",
      "loss: 49038.6875 832\n",
      "loss: 49023.21875 833\n",
      "loss: 49007.84765625 834\n",
      "loss: 48992.62109375 835\n",
      "loss: 48977.46875 836\n",
      "loss: 48962.4453125 837\n",
      "loss: 48947.5234375 838\n",
      "loss: 48932.7109375 839\n",
      "loss: 48918.0078125 840\n",
      "loss: 48903.41015625 841\n",
      "loss: 48888.90625 842\n",
      "loss: 48874.515625 843\n",
      "loss: 48860.23046875 844\n",
      "loss: 48846.046875 845\n",
      "loss: 48831.9609375 846\n",
      "loss: 48817.9765625 847\n",
      "loss: 48804.09765625 848\n",
      "loss: 48790.296875 849\n",
      "loss: 48776.6171875 850\n",
      "loss: 48763.01953125 851\n",
      "loss: 48749.53125 852\n",
      "loss: 48736.140625 853\n",
      "loss: 48722.8125 854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 48709.609375 855\n",
      "loss: 48696.4921875 856\n",
      "loss: 48683.4609375 857\n",
      "loss: 48670.5234375 858\n",
      "loss: 48657.69140625 859\n",
      "loss: 48644.9453125 860\n",
      "loss: 48632.28125 861\n",
      "loss: 48619.71875 862\n",
      "loss: 48607.22265625 863\n",
      "loss: 48594.8203125 864\n",
      "loss: 48582.52734375 865\n",
      "loss: 48570.2890625 866\n",
      "loss: 48558.15625 867\n",
      "loss: 48546.1015625 868\n",
      "loss: 48534.1328125 869\n",
      "loss: 48522.25390625 870\n",
      "loss: 48510.4375 871\n",
      "loss: 48498.71484375 872\n",
      "loss: 48487.07421875 873\n",
      "loss: 48475.52734375 874\n",
      "loss: 48464.046875 875\n",
      "loss: 48452.64453125 876\n",
      "loss: 48441.328125 877\n",
      "loss: 48430.078125 878\n",
      "loss: 48418.9140625 879\n",
      "loss: 48407.8203125 880\n",
      "loss: 48396.80859375 881\n",
      "loss: 48385.8671875 882\n",
      "loss: 48375.015625 883\n",
      "loss: 48364.21875 884\n",
      "loss: 48353.50390625 885\n",
      "loss: 48342.859375 886\n",
      "loss: 48332.29296875 887\n",
      "loss: 48321.79296875 888\n",
      "loss: 48311.375 889\n",
      "loss: 48301.01953125 890\n",
      "loss: 48290.734375 891\n",
      "loss: 48280.5234375 892\n",
      "loss: 48270.375 893\n",
      "loss: 48260.3046875 894\n",
      "loss: 48250.29296875 895\n",
      "loss: 48240.3515625 896\n",
      "loss: 48230.48046875 897\n",
      "loss: 48220.66796875 898\n",
      "loss: 48210.92578125 899\n",
      "loss: 48201.25 900\n",
      "loss: 48191.6484375 901\n",
      "loss: 48182.1015625 902\n",
      "loss: 48172.61328125 903\n",
      "loss: 48163.20703125 904\n",
      "loss: 48153.83203125 905\n",
      "loss: 48144.55078125 906\n",
      "loss: 48135.32421875 907\n",
      "loss: 48126.14453125 908\n",
      "loss: 48117.046875 909\n",
      "loss: 48107.9921875 910\n",
      "loss: 48098.98828125 911\n",
      "loss: 48090.0546875 912\n",
      "loss: 48081.1953125 913\n",
      "loss: 48072.375 914\n",
      "loss: 48063.625 915\n",
      "loss: 48054.93359375 916\n",
      "loss: 48046.2890625 917\n",
      "loss: 48037.69921875 918\n",
      "loss: 48029.171875 919\n",
      "loss: 48020.70703125 920\n",
      "loss: 48012.2890625 921\n",
      "loss: 48003.94140625 922\n",
      "loss: 47995.625 923\n",
      "loss: 47987.3671875 924\n",
      "loss: 47979.1796875 925\n",
      "loss: 47971.0234375 926\n",
      "loss: 47962.93359375 927\n",
      "loss: 47954.88671875 928\n",
      "loss: 47946.90234375 929\n",
      "loss: 47938.97265625 930\n",
      "loss: 47931.0859375 931\n",
      "loss: 47923.2421875 932\n",
      "loss: 47915.4609375 933\n",
      "loss: 47907.71875 934\n",
      "loss: 47900.0390625 935\n",
      "loss: 47892.3984375 936\n",
      "loss: 47884.8046875 937\n",
      "loss: 47877.2734375 938\n",
      "loss: 47869.7734375 939\n",
      "loss: 47862.328125 940\n",
      "loss: 47854.921875 941\n",
      "loss: 47847.58203125 942\n",
      "loss: 47840.265625 943\n",
      "loss: 47833.0078125 944\n",
      "loss: 47825.8046875 945\n",
      "loss: 47818.64453125 946\n",
      "loss: 47811.50390625 947\n",
      "loss: 47804.4375 948\n",
      "loss: 47797.3984375 949\n",
      "loss: 47790.4140625 950\n",
      "loss: 47783.46875 951\n",
      "loss: 47776.55078125 952\n",
      "loss: 47769.703125 953\n",
      "loss: 47762.875 954\n",
      "loss: 47756.109375 955\n",
      "loss: 47749.375 956\n",
      "loss: 47742.67578125 957\n",
      "loss: 47736.015625 958\n",
      "loss: 47729.4140625 959\n",
      "loss: 47722.84375 960\n",
      "loss: 47716.3203125 961\n",
      "loss: 47709.8203125 962\n",
      "loss: 47703.375 963\n",
      "loss: 47696.96875 964\n",
      "loss: 47690.59375 965\n",
      "loss: 47684.2578125 966\n",
      "loss: 47677.9609375 967\n",
      "loss: 47671.70703125 968\n",
      "loss: 47665.4921875 969\n",
      "loss: 47659.3125 970\n",
      "loss: 47653.15625 971\n",
      "loss: 47647.0546875 972\n",
      "loss: 47640.984375 973\n",
      "loss: 47634.953125 974\n",
      "loss: 47628.953125 975\n",
      "loss: 47622.9921875 976\n",
      "loss: 47617.05859375 977\n",
      "loss: 47611.171875 978\n",
      "loss: 47605.3125 979\n",
      "loss: 47599.49609375 980\n",
      "loss: 47593.6875 981\n",
      "loss: 47587.9453125 982\n",
      "loss: 47582.2265625 983\n",
      "loss: 47576.53515625 984\n",
      "loss: 47570.890625 985\n",
      "loss: 47565.26171875 986\n",
      "loss: 47559.67578125 987\n",
      "loss: 47554.11328125 988\n",
      "loss: 47548.6015625 989\n",
      "loss: 47543.09765625 990\n",
      "loss: 47537.65625 991\n",
      "loss: 47532.2109375 992\n",
      "loss: 47526.8203125 993\n",
      "loss: 47521.453125 994\n",
      "loss: 47516.125 995\n",
      "loss: 47510.8125 996\n",
      "loss: 47505.5390625 997\n",
      "loss: 47500.28125 998\n",
      "loss: 47495.0859375 999\n",
      "loss: 47489.88671875 1000\n",
      "loss: 47484.73828125 1001\n",
      "loss: 47479.6171875 1002\n",
      "loss: 47474.51171875 1003\n",
      "loss: 47469.44140625 1004\n",
      "loss: 47464.390625 1005\n",
      "loss: 47459.3828125 1006\n",
      "loss: 47454.40625 1007\n",
      "loss: 47449.44921875 1008\n",
      "loss: 47444.51171875 1009\n",
      "loss: 47439.6171875 1010\n",
      "loss: 47434.734375 1011\n",
      "loss: 47429.875 1012\n",
      "loss: 47425.05859375 1013\n",
      "loss: 47420.26953125 1014\n",
      "loss: 47415.5 1015\n",
      "loss: 47410.76171875 1016\n",
      "loss: 47406.04296875 1017\n",
      "loss: 47401.3515625 1018\n",
      "loss: 47396.6796875 1019\n",
      "loss: 47392.04296875 1020\n",
      "loss: 47387.4296875 1021\n",
      "loss: 47382.8359375 1022\n",
      "loss: 47378.26171875 1023\n",
      "loss: 47373.7265625 1024\n",
      "loss: 47369.203125 1025\n",
      "loss: 47364.71484375 1026\n",
      "loss: 47360.2421875 1027\n",
      "loss: 47355.796875 1028\n",
      "loss: 47351.3671875 1029\n",
      "loss: 47346.96875 1030\n",
      "loss: 47342.59375 1031\n",
      "loss: 47338.23828125 1032\n",
      "loss: 47333.9140625 1033\n",
      "loss: 47329.5859375 1034\n",
      "loss: 47325.3203125 1035\n",
      "loss: 47321.0546875 1036\n",
      "loss: 47316.8046875 1037\n",
      "loss: 47312.59375 1038\n",
      "loss: 47308.38671875 1039\n",
      "loss: 47304.21875 1040\n",
      "loss: 47300.0546875 1041\n",
      "loss: 47295.91796875 1042\n",
      "loss: 47291.8046875 1043\n",
      "loss: 47287.7109375 1044\n",
      "loss: 47283.6484375 1045\n",
      "loss: 47279.59375 1046\n",
      "loss: 47275.5703125 1047\n",
      "loss: 47271.546875 1048\n",
      "loss: 47267.5546875 1049\n",
      "loss: 47263.578125 1050\n",
      "loss: 47259.6328125 1051\n",
      "loss: 47255.703125 1052\n",
      "loss: 47251.7890625 1053\n",
      "loss: 47247.8984375 1054\n",
      "loss: 47244.03125 1055\n",
      "loss: 47240.1640625 1056\n",
      "loss: 47236.33203125 1057\n",
      "loss: 47232.515625 1058\n",
      "loss: 47228.7109375 1059\n",
      "loss: 47224.9375 1060\n",
      "loss: 47221.1796875 1061\n",
      "loss: 47217.421875 1062\n",
      "loss: 47213.6953125 1063\n",
      "loss: 47209.9921875 1064\n",
      "loss: 47206.296875 1065\n",
      "loss: 47202.6171875 1066\n",
      "loss: 47198.9765625 1067\n",
      "loss: 47195.3359375 1068\n",
      "loss: 47191.70703125 1069\n",
      "loss: 47188.09765625 1070\n",
      "loss: 47184.50390625 1071\n",
      "loss: 47180.9375 1072\n",
      "loss: 47177.3828125 1073\n",
      "loss: 47173.8515625 1074\n",
      "loss: 47170.3125 1075\n",
      "loss: 47166.8125 1076\n",
      "loss: 47163.328125 1077\n",
      "loss: 47159.84765625 1078\n",
      "loss: 47156.37890625 1079\n",
      "loss: 47152.9296875 1080\n",
      "loss: 47149.515625 1081\n",
      "loss: 47146.10546875 1082\n",
      "loss: 47142.703125 1083\n",
      "loss: 47139.3203125 1084\n",
      "loss: 47135.953125 1085\n",
      "loss: 47132.609375 1086\n",
      "loss: 47129.265625 1087\n",
      "loss: 47125.9375 1088\n",
      "loss: 47122.625 1089\n",
      "loss: 47119.328125 1090\n",
      "loss: 47116.0546875 1091\n",
      "loss: 47112.78125 1092\n",
      "loss: 47109.54296875 1093\n",
      "loss: 47106.30078125 1094\n",
      "loss: 47103.078125 1095\n",
      "loss: 47099.859375 1096\n",
      "loss: 47096.67578125 1097\n",
      "loss: 47093.484375 1098\n",
      "loss: 47090.32421875 1099\n",
      "loss: 47087.1640625 1100\n",
      "loss: 47084.015625 1101\n",
      "loss: 47080.88671875 1102\n",
      "loss: 47077.78125 1103\n",
      "loss: 47074.66796875 1104\n",
      "loss: 47071.5703125 1105\n",
      "loss: 47068.5 1106\n",
      "loss: 47065.4296875 1107\n",
      "loss: 47062.3828125 1108\n",
      "loss: 47059.3359375 1109\n",
      "loss: 47056.30078125 1110\n",
      "loss: 47053.28515625 1111\n",
      "loss: 47050.28125 1112\n",
      "loss: 47047.29296875 1113\n",
      "loss: 47044.3046875 1114\n",
      "loss: 47041.328125 1115\n",
      "loss: 47038.37109375 1116\n",
      "loss: 47035.421875 1117\n",
      "loss: 47032.5 1118\n",
      "loss: 47029.5625 1119\n",
      "loss: 47026.6640625 1120\n",
      "loss: 47023.765625 1121\n",
      "loss: 47020.8671875 1122\n",
      "loss: 47017.9921875 1123\n",
      "loss: 47015.1171875 1124\n",
      "loss: 47012.25 1125\n",
      "loss: 47009.4140625 1126\n",
      "loss: 47006.5625 1127\n",
      "loss: 47003.73828125 1128\n",
      "loss: 47000.93359375 1129\n",
      "loss: 46998.1328125 1130\n",
      "loss: 46995.328125 1131\n",
      "loss: 46992.546875 1132\n",
      "loss: 46989.765625 1133\n",
      "loss: 46987.0 1134\n",
      "loss: 46984.24609375 1135\n",
      "loss: 46981.5078125 1136\n",
      "loss: 46978.765625 1137\n",
      "loss: 46976.046875 1138\n",
      "loss: 46973.328125 1139\n",
      "loss: 46970.6171875 1140\n",
      "loss: 46967.921875 1141\n",
      "loss: 46965.2421875 1142\n",
      "loss: 46962.546875 1143\n",
      "loss: 46959.890625 1144\n",
      "loss: 46957.234375 1145\n",
      "loss: 46954.5859375 1146\n",
      "loss: 46951.9375 1147\n",
      "loss: 46949.3046875 1148\n",
      "loss: 46946.68359375 1149\n",
      "loss: 46944.0703125 1150\n",
      "loss: 46941.453125 1151\n",
      "loss: 46938.85546875 1152\n",
      "loss: 46936.2734375 1153\n",
      "loss: 46933.69921875 1154\n",
      "loss: 46931.1171875 1155\n",
      "loss: 46928.546875 1156\n",
      "loss: 46926.0 1157\n",
      "loss: 46923.4453125 1158\n",
      "loss: 46920.90234375 1159\n",
      "loss: 46918.375 1160\n",
      "loss: 46915.8515625 1161\n",
      "loss: 46913.32421875 1162\n",
      "loss: 46910.828125 1163\n",
      "loss: 46908.328125 1164\n",
      "loss: 46905.828125 1165\n",
      "loss: 46903.3515625 1166\n",
      "loss: 46900.8671875 1167\n",
      "loss: 46898.390625 1168\n",
      "loss: 46895.9296875 1169\n",
      "loss: 46893.4765625 1170\n",
      "loss: 46891.01953125 1171\n",
      "loss: 46888.58203125 1172\n",
      "loss: 46886.15625 1173\n",
      "loss: 46883.7265625 1174\n",
      "loss: 46881.3046875 1175\n",
      "loss: 46878.890625 1176\n",
      "loss: 46876.484375 1177\n",
      "loss: 46874.09375 1178\n",
      "loss: 46871.69921875 1179\n",
      "loss: 46869.3125 1180\n",
      "loss: 46866.93359375 1181\n",
      "loss: 46864.5546875 1182\n",
      "loss: 46862.1875 1183\n",
      "loss: 46859.8203125 1184\n",
      "loss: 46857.4765625 1185\n",
      "loss: 46855.1328125 1186\n",
      "loss: 46852.7890625 1187\n",
      "loss: 46850.453125 1188\n",
      "loss: 46848.125 1189\n",
      "loss: 46845.796875 1190\n",
      "loss: 46843.484375 1191\n",
      "loss: 46841.1796875 1192\n",
      "loss: 46838.88671875 1193\n",
      "loss: 46836.578125 1194\n",
      "loss: 46834.29296875 1195\n",
      "loss: 46831.99609375 1196\n",
      "loss: 46829.71484375 1197\n",
      "loss: 46827.4453125 1198\n",
      "loss: 46825.16796875 1199\n",
      "loss: 46822.921875 1200\n",
      "loss: 46820.6640625 1201\n",
      "loss: 46818.3984375 1202\n",
      "loss: 46816.1484375 1203\n",
      "loss: 46813.90625 1204\n",
      "loss: 46811.671875 1205\n",
      "loss: 46809.4453125 1206\n",
      "loss: 46807.20703125 1207\n",
      "loss: 46804.9921875 1208\n",
      "loss: 46802.7890625 1209\n",
      "loss: 46800.578125 1210\n",
      "loss: 46798.35546875 1211\n",
      "loss: 46796.15625 1212\n",
      "loss: 46793.9609375 1213\n",
      "loss: 46791.78125 1214\n",
      "loss: 46789.5859375 1215\n",
      "loss: 46787.3984375 1216\n",
      "loss: 46785.2265625 1217\n",
      "loss: 46783.0546875 1218\n",
      "loss: 46780.8984375 1219\n",
      "loss: 46778.734375 1220\n",
      "loss: 46776.56640625 1221\n",
      "loss: 46774.4140625 1222\n",
      "loss: 46772.27734375 1223\n",
      "loss: 46770.140625 1224\n",
      "loss: 46767.9921875 1225\n",
      "loss: 46765.8515625 1226\n",
      "loss: 46763.71484375 1227\n",
      "loss: 46761.58984375 1228\n",
      "loss: 46759.46875 1229\n",
      "loss: 46757.34375 1230\n",
      "loss: 46755.23828125 1231\n",
      "loss: 46753.11328125 1232\n",
      "loss: 46751.0234375 1233\n",
      "loss: 46748.91796875 1234\n",
      "loss: 46746.80859375 1235\n",
      "loss: 46744.71875 1236\n",
      "loss: 46742.62890625 1237\n",
      "loss: 46740.54296875 1238\n",
      "loss: 46738.4453125 1239\n",
      "loss: 46736.375 1240\n",
      "loss: 46734.30859375 1241\n",
      "loss: 46732.23828125 1242\n",
      "loss: 46730.16015625 1243\n",
      "loss: 46728.09765625 1244\n",
      "loss: 46726.0390625 1245\n",
      "loss: 46723.9765625 1246\n",
      "loss: 46721.9140625 1247\n",
      "loss: 46719.87890625 1248\n",
      "loss: 46717.82421875 1249\n",
      "loss: 46715.78515625 1250\n",
      "loss: 46713.7421875 1251\n",
      "loss: 46711.6953125 1252\n",
      "loss: 46709.671875 1253\n",
      "loss: 46707.640625 1254\n",
      "loss: 46705.6015625 1255\n",
      "loss: 46703.578125 1256\n",
      "loss: 46701.56640625 1257\n",
      "loss: 46699.546875 1258\n",
      "loss: 46697.52734375 1259\n",
      "loss: 46695.51171875 1260\n",
      "loss: 46693.5078125 1261\n",
      "loss: 46691.50390625 1262\n",
      "loss: 46689.5 1263\n",
      "loss: 46687.49609375 1264\n",
      "loss: 46685.4921875 1265\n",
      "loss: 46683.5 1266\n",
      "loss: 46681.51171875 1267\n",
      "loss: 46679.515625 1268\n",
      "loss: 46677.5234375 1269\n",
      "loss: 46675.546875 1270\n",
      "loss: 46673.5625 1271\n",
      "loss: 46671.58984375 1272\n",
      "loss: 46669.6171875 1273\n",
      "loss: 46667.6328125 1274\n",
      "loss: 46665.6640625 1275\n",
      "loss: 46663.6953125 1276\n",
      "loss: 46661.7265625 1277\n",
      "loss: 46659.7578125 1278\n",
      "loss: 46657.8046875 1279\n",
      "loss: 46655.8515625 1280\n",
      "loss: 46653.8828125 1281\n",
      "loss: 46651.921875 1282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 46649.97265625 1283\n",
      "loss: 46648.0234375 1284\n",
      "loss: 46646.08203125 1285\n",
      "loss: 46644.1328125 1286\n",
      "loss: 46642.1796875 1287\n",
      "loss: 46640.25 1288\n",
      "loss: 46638.3046875 1289\n",
      "loss: 46636.3828125 1290\n",
      "loss: 46634.4296875 1291\n",
      "loss: 46632.5 1292\n",
      "loss: 46630.5625 1293\n",
      "loss: 46628.6328125 1294\n",
      "loss: 46626.609375 1295\n",
      "loss: 46610.93359375 1296\n",
      "loss: 46599.0625 1297\n",
      "loss: 46588.8828125 1298\n",
      "loss: 46579.7265625 1299\n",
      "loss: 46571.2578125 1300\n",
      "loss: 46563.265625 1301\n",
      "loss: 46555.671875 1302\n",
      "loss: 46548.359375 1303\n",
      "loss: 46541.2734375 1304\n",
      "loss: 46534.40234375 1305\n",
      "loss: 46527.671875 1306\n",
      "loss: 46521.0859375 1307\n",
      "loss: 46514.609375 1308\n",
      "loss: 46508.234375 1309\n",
      "loss: 46501.953125 1310\n",
      "loss: 46495.7265625 1311\n",
      "loss: 46489.578125 1312\n",
      "loss: 46483.484375 1313\n",
      "loss: 46477.4375 1314\n",
      "loss: 46471.44140625 1315\n",
      "loss: 46465.48046875 1316\n",
      "loss: 46459.55078125 1317\n",
      "loss: 46453.671875 1318\n",
      "loss: 46447.8046875 1319\n",
      "loss: 46441.96875 1320\n",
      "loss: 46436.16015625 1321\n",
      "loss: 46430.37890625 1322\n",
      "loss: 46424.6171875 1323\n",
      "loss: 46418.8671875 1324\n",
      "loss: 46413.140625 1325\n",
      "loss: 46407.421875 1326\n",
      "loss: 46401.7265625 1327\n",
      "loss: 46396.03515625 1328\n",
      "loss: 46390.359375 1329\n",
      "loss: 46384.70703125 1330\n",
      "loss: 46379.0625 1331\n",
      "loss: 46373.4296875 1332\n",
      "loss: 46367.796875 1333\n",
      "loss: 46362.18359375 1334\n",
      "loss: 46356.56640625 1335\n",
      "loss: 46350.984375 1336\n",
      "loss: 46345.390625 1337\n",
      "loss: 46339.8125 1338\n",
      "loss: 46334.234375 1339\n",
      "loss: 46328.65625 1340\n",
      "loss: 46323.09765625 1341\n",
      "loss: 46317.546875 1342\n",
      "loss: 46312.00390625 1343\n",
      "loss: 46306.46484375 1344\n",
      "loss: 46300.9296875 1345\n",
      "loss: 46295.40625 1346\n",
      "loss: 46289.8671875 1347\n",
      "loss: 46284.359375 1348\n",
      "loss: 46278.84765625 1349\n",
      "loss: 46273.35546875 1350\n",
      "loss: 46267.84375 1351\n",
      "loss: 46262.34765625 1352\n",
      "loss: 46256.84765625 1353\n",
      "loss: 46251.359375 1354\n",
      "loss: 46245.890625 1355\n",
      "loss: 46240.4140625 1356\n",
      "loss: 46234.93359375 1357\n",
      "loss: 46229.46484375 1358\n",
      "loss: 46223.984375 1359\n",
      "loss: 46218.53125 1360\n",
      "loss: 46213.0859375 1361\n",
      "loss: 46207.6328125 1362\n",
      "loss: 46202.171875 1363\n",
      "loss: 46196.734375 1364\n",
      "loss: 46191.2890625 1365\n",
      "loss: 46185.84765625 1366\n",
      "loss: 46180.4140625 1367\n",
      "loss: 46174.984375 1368\n",
      "loss: 46169.546875 1369\n",
      "loss: 46164.1328125 1370\n",
      "loss: 46158.703125 1371\n",
      "loss: 46153.28125 1372\n",
      "loss: 46147.8671875 1373\n",
      "loss: 46142.4453125 1374\n",
      "loss: 46137.046875 1375\n",
      "loss: 46131.64453125 1376\n",
      "loss: 46126.23046875 1377\n",
      "loss: 46120.828125 1378\n",
      "loss: 46115.4375 1379\n",
      "loss: 46110.046875 1380\n",
      "loss: 46104.6484375 1381\n",
      "loss: 46099.25 1382\n",
      "loss: 46093.875 1383\n",
      "loss: 46088.48046875 1384\n",
      "loss: 46083.09765625 1385\n",
      "loss: 46077.7265625 1386\n",
      "loss: 46072.34375 1387\n",
      "loss: 46066.9609375 1388\n",
      "loss: 46061.5859375 1389\n",
      "loss: 46056.22265625 1390\n",
      "loss: 46050.859375 1391\n",
      "loss: 46045.4921875 1392\n",
      "loss: 46040.12890625 1393\n",
      "loss: 46034.76953125 1394\n",
      "loss: 46029.41015625 1395\n",
      "loss: 46024.05078125 1396\n",
      "loss: 46018.69140625 1397\n",
      "loss: 46013.3359375 1398\n",
      "loss: 46008.0 1399\n",
      "loss: 46002.6328125 1400\n",
      "loss: 45997.296875 1401\n",
      "loss: 45991.953125 1402\n",
      "loss: 45986.6015625 1403\n",
      "loss: 45981.2734375 1404\n",
      "loss: 45975.94140625 1405\n",
      "loss: 45970.59375 1406\n",
      "loss: 45965.26171875 1407\n",
      "loss: 45959.92578125 1408\n",
      "loss: 45954.59375 1409\n",
      "loss: 45949.27734375 1410\n",
      "loss: 45943.953125 1411\n",
      "loss: 45938.609375 1412\n",
      "loss: 45933.2890625 1413\n",
      "loss: 45927.96484375 1414\n",
      "loss: 45922.64453125 1415\n",
      "loss: 45917.3203125 1416\n",
      "loss: 45912.0078125 1417\n",
      "loss: 45906.6953125 1418\n",
      "loss: 45901.3828125 1419\n",
      "loss: 45896.0703125 1420\n",
      "loss: 45890.75 1421\n",
      "loss: 45885.43359375 1422\n",
      "loss: 45880.13671875 1423\n",
      "loss: 45874.8203125 1424\n",
      "loss: 45869.515625 1425\n",
      "loss: 45864.21875 1426\n",
      "loss: 45858.9140625 1427\n",
      "loss: 45853.61328125 1428\n",
      "loss: 45848.3203125 1429\n",
      "loss: 45843.015625 1430\n",
      "loss: 45837.71484375 1431\n",
      "loss: 45832.42578125 1432\n",
      "loss: 45827.12890625 1433\n",
      "loss: 45821.84375 1434\n",
      "loss: 45816.5390625 1435\n",
      "loss: 45811.23828125 1436\n",
      "loss: 45805.9609375 1437\n",
      "loss: 45800.671875 1438\n",
      "loss: 45795.3828125 1439\n",
      "loss: 45790.1015625 1440\n",
      "loss: 45784.81640625 1441\n",
      "loss: 45779.53125 1442\n",
      "loss: 45774.25390625 1443\n",
      "loss: 45768.96875 1444\n",
      "loss: 45763.6796875 1445\n",
      "loss: 45758.3984375 1446\n",
      "loss: 45753.12109375 1447\n",
      "loss: 45747.84765625 1448\n",
      "loss: 45742.5703125 1449\n",
      "loss: 45737.30078125 1450\n",
      "loss: 45732.0234375 1451\n",
      "loss: 45726.75 1452\n",
      "loss: 45721.48046875 1453\n",
      "loss: 45716.20703125 1454\n",
      "loss: 45710.9296875 1455\n",
      "loss: 45705.66796875 1456\n",
      "loss: 45700.3984375 1457\n",
      "loss: 45695.125 1458\n",
      "loss: 45689.8671875 1459\n",
      "loss: 45684.5859375 1460\n",
      "loss: 45679.31640625 1461\n",
      "loss: 45674.0546875 1462\n",
      "loss: 45668.7890625 1463\n",
      "loss: 45663.5234375 1464\n",
      "loss: 45658.2578125 1465\n",
      "loss: 45653.0 1466\n",
      "loss: 45647.734375 1467\n",
      "loss: 45642.48828125 1468\n",
      "loss: 45637.22265625 1469\n",
      "loss: 45631.94921875 1470\n",
      "loss: 45626.69921875 1471\n",
      "loss: 45621.421875 1472\n",
      "loss: 45616.1640625 1473\n",
      "loss: 45610.921875 1474\n",
      "loss: 45605.65625 1475\n",
      "loss: 45600.3984375 1476\n",
      "loss: 45595.15234375 1477\n",
      "loss: 45589.89453125 1478\n",
      "loss: 45584.63671875 1479\n",
      "loss: 45579.3828125 1480\n",
      "loss: 45574.13671875 1481\n",
      "loss: 45568.890625 1482\n",
      "loss: 45563.62109375 1483\n",
      "loss: 45558.375 1484\n",
      "loss: 45553.1171875 1485\n",
      "loss: 45547.86328125 1486\n",
      "loss: 45542.6171875 1487\n",
      "loss: 45537.35546875 1488\n",
      "loss: 45532.1171875 1489\n",
      "loss: 45526.859375 1490\n",
      "loss: 45521.6171875 1491\n",
      "loss: 45516.359375 1492\n",
      "loss: 45511.1171875 1493\n",
      "loss: 45505.86328125 1494\n",
      "loss: 45500.6171875 1495\n",
      "loss: 45495.3828125 1496\n",
      "loss: 45490.1328125 1497\n",
      "loss: 45484.875 1498\n",
      "loss: 45479.6328125 1499\n",
      "loss: 45474.37890625 1500\n",
      "loss: 45469.140625 1501\n",
      "loss: 45463.890625 1502\n",
      "loss: 45458.6484375 1503\n",
      "loss: 45453.3984375 1504\n",
      "loss: 45448.15234375 1505\n",
      "loss: 45442.90625 1506\n",
      "loss: 45437.6640625 1507\n",
      "loss: 45432.41796875 1508\n",
      "loss: 45427.171875 1509\n",
      "loss: 45421.9296875 1510\n",
      "loss: 45416.6875 1511\n",
      "loss: 45411.4375 1512\n",
      "loss: 45406.1875 1513\n",
      "loss: 45400.9453125 1514\n",
      "loss: 45395.71484375 1515\n",
      "loss: 45390.45703125 1516\n",
      "loss: 45385.2109375 1517\n",
      "loss: 45379.9609375 1518\n",
      "loss: 45374.7265625 1519\n",
      "loss: 45369.4921875 1520\n",
      "loss: 45364.2421875 1521\n",
      "loss: 45359.0 1522\n",
      "loss: 45353.75 1523\n",
      "loss: 45348.51171875 1524\n",
      "loss: 45343.265625 1525\n",
      "loss: 45338.03515625 1526\n",
      "loss: 45332.7734375 1527\n",
      "loss: 45327.53125 1528\n",
      "loss: 45322.2890625 1529\n",
      "loss: 45317.0546875 1530\n",
      "loss: 45311.80859375 1531\n",
      "loss: 45306.5546875 1532\n",
      "loss: 45301.3203125 1533\n",
      "loss: 45296.0703125 1534\n",
      "loss: 45290.828125 1535\n",
      "loss: 45285.5703125 1536\n",
      "loss: 45280.33203125 1537\n",
      "loss: 45275.0859375 1538\n",
      "loss: 45269.84375 1539\n",
      "loss: 45264.6015625 1540\n",
      "loss: 45259.3515625 1541\n",
      "loss: 45254.109375 1542\n",
      "loss: 45248.8828125 1543\n",
      "loss: 45243.6171875 1544\n",
      "loss: 45238.3671875 1545\n",
      "loss: 45233.12109375 1546\n",
      "loss: 45227.875 1547\n",
      "loss: 45222.63671875 1548\n",
      "loss: 45217.3828125 1549\n",
      "loss: 45212.1484375 1550\n",
      "loss: 45206.8828125 1551\n",
      "loss: 45201.6484375 1552\n",
      "loss: 45196.40234375 1553\n",
      "loss: 45191.14453125 1554\n",
      "loss: 45185.90234375 1555\n",
      "loss: 45180.65625 1556\n",
      "loss: 45175.40625 1557\n",
      "loss: 45170.1484375 1558\n",
      "loss: 45164.8984375 1559\n",
      "loss: 45159.6484375 1560\n",
      "loss: 45154.40625 1561\n",
      "loss: 45149.15625 1562\n",
      "loss: 45143.90234375 1563\n",
      "loss: 45138.640625 1564\n",
      "loss: 45133.3984375 1565\n",
      "loss: 45128.13671875 1566\n",
      "loss: 45122.90234375 1567\n",
      "loss: 45117.6328125 1568\n",
      "loss: 45112.390625 1569\n",
      "loss: 45107.1328125 1570\n",
      "loss: 45101.875 1571\n",
      "loss: 45096.62109375 1572\n",
      "loss: 45091.37890625 1573\n",
      "loss: 45086.109375 1574\n",
      "loss: 45080.86328125 1575\n",
      "loss: 45075.6015625 1576\n",
      "loss: 45070.34375 1577\n",
      "loss: 45065.078125 1578\n",
      "loss: 45059.828125 1579\n",
      "loss: 45054.57421875 1580\n",
      "loss: 45049.3125 1581\n",
      "loss: 45044.046875 1582\n",
      "loss: 45038.7890625 1583\n",
      "loss: 45033.53515625 1584\n",
      "loss: 45028.2734375 1585\n",
      "loss: 45023.015625 1586\n",
      "loss: 45017.74609375 1587\n",
      "loss: 45012.4921875 1588\n",
      "loss: 45007.21875 1589\n",
      "loss: 45001.9609375 1590\n",
      "loss: 44996.67578125 1591\n",
      "loss: 44991.41796875 1592\n",
      "loss: 44986.15625 1593\n",
      "loss: 44980.8984375 1594\n",
      "loss: 44975.62890625 1595\n",
      "loss: 44970.35546875 1596\n",
      "loss: 44965.0859375 1597\n",
      "loss: 44959.828125 1598\n",
      "loss: 44954.55078125 1599\n",
      "loss: 44949.28515625 1600\n",
      "loss: 44944.015625 1601\n",
      "loss: 44938.7421875 1602\n",
      "loss: 44933.4765625 1603\n",
      "loss: 44928.1875 1604\n",
      "loss: 44922.9296875 1605\n",
      "loss: 44917.6484375 1606\n",
      "loss: 44912.3671875 1607\n",
      "loss: 44907.10546875 1608\n",
      "loss: 44901.83203125 1609\n",
      "loss: 44896.5546875 1610\n",
      "loss: 44891.28125 1611\n",
      "loss: 44886.0 1612\n",
      "loss: 44880.71875 1613\n",
      "loss: 44875.4375 1614\n",
      "loss: 44870.15625 1615\n",
      "loss: 44864.87109375 1616\n",
      "loss: 44859.59375 1617\n",
      "loss: 44854.3203125 1618\n",
      "loss: 44849.03125 1619\n",
      "loss: 44843.7421875 1620\n",
      "loss: 44838.4609375 1621\n",
      "loss: 44833.1640625 1622\n",
      "loss: 44827.88671875 1623\n",
      "loss: 44822.59375 1624\n",
      "loss: 44817.30859375 1625\n",
      "loss: 44812.02734375 1626\n",
      "loss: 44806.734375 1627\n",
      "loss: 44801.4375 1628\n",
      "loss: 44796.1484375 1629\n",
      "loss: 44790.859375 1630\n",
      "loss: 44785.5625 1631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 44780.265625 1632\n",
      "loss: 44774.97265625 1633\n",
      "loss: 44769.6796875 1634\n",
      "loss: 44764.375 1635\n",
      "loss: 44759.0859375 1636\n",
      "loss: 44753.7890625 1637\n",
      "loss: 44748.484375 1638\n",
      "loss: 44743.1875 1639\n",
      "loss: 44737.875 1640\n",
      "loss: 44732.578125 1641\n",
      "loss: 44727.2734375 1642\n",
      "loss: 44721.96484375 1643\n",
      "loss: 44716.66796875 1644\n",
      "loss: 44711.359375 1645\n",
      "loss: 44706.05859375 1646\n",
      "loss: 44700.75390625 1647\n",
      "loss: 44695.4375 1648\n",
      "loss: 44690.125 1649\n",
      "loss: 44684.8203125 1650\n",
      "loss: 44679.515625 1651\n",
      "loss: 44674.19921875 1652\n",
      "loss: 44668.8828125 1653\n",
      "loss: 44663.57421875 1654\n",
      "loss: 44658.24609375 1655\n",
      "loss: 44652.9375 1656\n",
      "loss: 44647.6171875 1657\n",
      "loss: 44642.3046875 1658\n",
      "loss: 44636.984375 1659\n",
      "loss: 44631.65625 1660\n",
      "loss: 44626.33203125 1661\n",
      "loss: 44621.015625 1662\n",
      "loss: 44615.6953125 1663\n",
      "loss: 44610.37109375 1664\n",
      "loss: 44605.046875 1665\n",
      "loss: 44599.71484375 1666\n",
      "loss: 44594.37890625 1667\n",
      "loss: 44589.0703125 1668\n",
      "loss: 44583.73828125 1669\n",
      "loss: 44578.40234375 1670\n",
      "loss: 44573.06640625 1671\n",
      "loss: 44567.73828125 1672\n",
      "loss: 44562.3984375 1673\n",
      "loss: 44557.0703125 1674\n",
      "loss: 44551.7265625 1675\n",
      "loss: 44546.390625 1676\n",
      "loss: 44541.046875 1677\n",
      "loss: 44535.703125 1678\n",
      "loss: 44530.37109375 1679\n",
      "loss: 44525.0390625 1680\n",
      "loss: 44519.6796875 1681\n",
      "loss: 44514.3359375 1682\n",
      "loss: 44509.0078125 1683\n",
      "loss: 44503.65234375 1684\n",
      "loss: 44498.30859375 1685\n",
      "loss: 44492.9609375 1686\n",
      "loss: 44487.609375 1687\n",
      "loss: 44482.25 1688\n",
      "loss: 44476.9140625 1689\n",
      "loss: 44471.546875 1690\n",
      "loss: 44466.1875 1691\n",
      "loss: 44460.828125 1692\n",
      "loss: 44455.4765625 1693\n",
      "loss: 44450.125 1694\n",
      "loss: 44444.75390625 1695\n",
      "loss: 44439.3984375 1696\n",
      "loss: 44434.0390625 1697\n",
      "loss: 44428.671875 1698\n",
      "loss: 44423.3125 1699\n",
      "loss: 44417.953125 1700\n",
      "loss: 44412.5859375 1701\n",
      "loss: 44407.20703125 1702\n",
      "loss: 44401.8359375 1703\n",
      "loss: 44396.4609375 1704\n",
      "loss: 44391.09375 1705\n",
      "loss: 44385.73046875 1706\n",
      "loss: 44380.3359375 1707\n",
      "loss: 44374.96875 1708\n",
      "loss: 44369.59765625 1709\n",
      "loss: 44364.2109375 1710\n",
      "loss: 44358.8359375 1711\n",
      "loss: 44353.4609375 1712\n",
      "loss: 44348.0703125 1713\n",
      "loss: 44342.69921875 1714\n",
      "loss: 44337.296875 1715\n",
      "loss: 44331.921875 1716\n",
      "loss: 44326.54296875 1717\n",
      "loss: 44321.15234375 1718\n",
      "loss: 44315.7578125 1719\n",
      "loss: 44310.375 1720\n",
      "loss: 44304.9765625 1721\n",
      "loss: 44299.5859375 1722\n",
      "loss: 44294.18359375 1723\n",
      "loss: 44288.7890625 1724\n",
      "loss: 44283.3828125 1725\n",
      "loss: 44277.9921875 1726\n",
      "loss: 44272.5859375 1727\n",
      "loss: 44267.18359375 1728\n",
      "loss: 44261.78515625 1729\n",
      "loss: 44256.3671875 1730\n",
      "loss: 44250.96875 1731\n",
      "loss: 44245.5625 1732\n",
      "loss: 44240.1484375 1733\n",
      "loss: 44234.7421875 1734\n",
      "loss: 44229.328125 1735\n",
      "loss: 44223.91015625 1736\n",
      "loss: 44218.5 1737\n",
      "loss: 44213.0859375 1738\n",
      "loss: 44207.671875 1739\n",
      "loss: 44202.0 1740\n",
      "loss: 44174.84375 1741\n",
      "loss: 44153.765625 1742\n",
      "loss: 44135.4765625 1743\n",
      "loss: 44118.84765625 1744\n",
      "loss: 44103.3828125 1745\n",
      "loss: 44088.734375 1746\n",
      "loss: 44074.7578125 1747\n",
      "loss: 44061.30078125 1748\n",
      "loss: 44048.2734375 1749\n",
      "loss: 44035.61328125 1750\n",
      "loss: 44023.265625 1751\n",
      "loss: 44011.1875 1752\n",
      "loss: 43999.33203125 1753\n",
      "loss: 43987.7109375 1754\n",
      "loss: 43976.2578125 1755\n",
      "loss: 43964.9765625 1756\n",
      "loss: 43953.86328125 1757\n",
      "loss: 43942.87890625 1758\n",
      "loss: 43932.01171875 1759\n",
      "loss: 43921.27734375 1760\n",
      "loss: 43910.640625 1761\n",
      "loss: 43900.1015625 1762\n",
      "loss: 43889.65625 1763\n",
      "loss: 43879.28125 1764\n",
      "loss: 43869.015625 1765\n",
      "loss: 43858.8125 1766\n",
      "loss: 43848.671875 1767\n",
      "loss: 43838.6015625 1768\n",
      "loss: 43828.59375 1769\n",
      "loss: 43818.640625 1770\n",
      "loss: 43808.75 1771\n",
      "loss: 43798.90625 1772\n",
      "loss: 43789.125 1773\n",
      "loss: 43779.37890625 1774\n",
      "loss: 43769.68359375 1775\n",
      "loss: 43760.0234375 1776\n",
      "loss: 43750.3984375 1777\n",
      "loss: 43740.83203125 1778\n",
      "loss: 43731.28125 1779\n",
      "loss: 43721.7734375 1780\n",
      "loss: 43712.296875 1781\n",
      "loss: 43702.859375 1782\n",
      "loss: 43693.453125 1783\n",
      "loss: 43684.0859375 1784\n",
      "loss: 43674.72265625 1785\n",
      "loss: 43665.40625 1786\n",
      "loss: 43656.10546875 1787\n",
      "loss: 43646.8359375 1788\n",
      "loss: 43637.58203125 1789\n",
      "loss: 43628.3515625 1790\n",
      "loss: 43619.15625 1791\n",
      "loss: 43609.98046875 1792\n",
      "loss: 43600.8125 1793\n",
      "loss: 43591.6796875 1794\n",
      "loss: 43582.5703125 1795\n",
      "loss: 43573.46875 1796\n",
      "loss: 43564.3828125 1797\n",
      "loss: 43555.3125 1798\n",
      "loss: 43546.26171875 1799\n",
      "loss: 43537.25 1800\n",
      "loss: 43528.22265625 1801\n",
      "loss: 43519.2421875 1802\n",
      "loss: 43510.25 1803\n",
      "loss: 43501.27734375 1804\n",
      "loss: 43492.33203125 1805\n",
      "loss: 43483.3828125 1806\n",
      "loss: 43474.46484375 1807\n",
      "loss: 43465.546875 1808\n",
      "loss: 43456.6484375 1809\n",
      "loss: 43447.75 1810\n",
      "loss: 43438.875 1811\n",
      "loss: 43430.015625 1812\n",
      "loss: 43421.1640625 1813\n",
      "loss: 43412.32421875 1814\n",
      "loss: 43403.48046875 1815\n",
      "loss: 43394.65625 1816\n",
      "loss: 43385.859375 1817\n",
      "loss: 43377.0546875 1818\n",
      "loss: 43368.26953125 1819\n",
      "loss: 43359.4765625 1820\n",
      "loss: 43350.703125 1821\n",
      "loss: 43341.9453125 1822\n",
      "loss: 43333.18359375 1823\n",
      "loss: 43324.4453125 1824\n",
      "loss: 43315.703125 1825\n",
      "loss: 43306.98046875 1826\n",
      "loss: 43298.2578125 1827\n",
      "loss: 43289.53125 1828\n",
      "loss: 43280.83203125 1829\n",
      "loss: 43272.125 1830\n",
      "loss: 43263.4375 1831\n",
      "loss: 43254.75 1832\n",
      "loss: 43246.0625 1833\n",
      "loss: 43237.3984375 1834\n",
      "loss: 43228.72265625 1835\n",
      "loss: 43220.0703125 1836\n",
      "loss: 43211.4296875 1837\n",
      "loss: 43202.765625 1838\n",
      "loss: 43194.12890625 1839\n",
      "loss: 43185.49609375 1840\n",
      "loss: 43176.859375 1841\n",
      "loss: 43168.234375 1842\n",
      "loss: 43159.6171875 1843\n",
      "loss: 43150.99609375 1844\n",
      "loss: 43142.40625 1845\n",
      "loss: 43133.796875 1846\n",
      "loss: 43125.19921875 1847\n",
      "loss: 43116.60546875 1848\n",
      "loss: 43108.01171875 1849\n",
      "loss: 43099.4375 1850\n",
      "loss: 43090.84375 1851\n",
      "loss: 43082.28125 1852\n",
      "loss: 43073.69921875 1853\n",
      "loss: 43065.140625 1854\n",
      "loss: 43056.57421875 1855\n",
      "loss: 43048.0234375 1856\n",
      "loss: 43039.46875 1857\n",
      "loss: 43030.91796875 1858\n",
      "loss: 43022.375 1859\n",
      "loss: 43013.83203125 1860\n",
      "loss: 43005.29296875 1861\n",
      "loss: 42996.7578125 1862\n",
      "loss: 42988.23828125 1863\n",
      "loss: 42979.6953125 1864\n",
      "loss: 42971.1796875 1865\n",
      "loss: 42962.640625 1866\n",
      "loss: 42954.1328125 1867\n",
      "loss: 42945.61328125 1868\n",
      "loss: 42937.1015625 1869\n",
      "loss: 42928.5859375 1870\n",
      "loss: 42920.08203125 1871\n",
      "loss: 42911.58984375 1872\n",
      "loss: 42903.08203125 1873\n",
      "loss: 42894.59375 1874\n",
      "loss: 42886.0859375 1875\n",
      "loss: 42877.609375 1876\n",
      "loss: 42869.11328125 1877\n",
      "loss: 42860.6328125 1878\n",
      "loss: 42852.140625 1879\n",
      "loss: 42843.6640625 1880\n",
      "loss: 42835.1796875 1881\n",
      "loss: 42826.7109375 1882\n",
      "loss: 42818.234375 1883\n",
      "loss: 42809.765625 1884\n",
      "loss: 42801.2890625 1885\n",
      "loss: 42792.8203125 1886\n",
      "loss: 42784.359375 1887\n",
      "loss: 42775.90234375 1888\n",
      "loss: 42767.4296875 1889\n",
      "loss: 42758.97265625 1890\n",
      "loss: 42750.51953125 1891\n",
      "loss: 42742.0546875 1892\n",
      "loss: 42733.59765625 1893\n",
      "loss: 42725.15234375 1894\n",
      "loss: 42716.7109375 1895\n",
      "loss: 42708.265625 1896\n",
      "loss: 42699.8203125 1897\n",
      "loss: 42691.36328125 1898\n",
      "loss: 42682.91796875 1899\n",
      "loss: 42674.48828125 1900\n",
      "loss: 42666.03515625 1901\n",
      "loss: 42657.609375 1902\n",
      "loss: 42649.1640625 1903\n",
      "loss: 42640.734375 1904\n",
      "loss: 42632.30078125 1905\n",
      "loss: 42623.87109375 1906\n",
      "loss: 42615.4296875 1907\n",
      "loss: 42607.01171875 1908\n",
      "loss: 42598.58203125 1909\n",
      "loss: 42590.15234375 1910\n",
      "loss: 42581.7265625 1911\n",
      "loss: 42573.29296875 1912\n",
      "loss: 42564.8828125 1913\n",
      "loss: 42556.4453125 1914\n",
      "loss: 42548.0390625 1915\n",
      "loss: 42539.61328125 1916\n",
      "loss: 42531.19140625 1917\n",
      "loss: 42522.77734375 1918\n",
      "loss: 42514.359375 1919\n",
      "loss: 42505.9453125 1920\n",
      "loss: 42497.53125 1921\n",
      "loss: 42489.1171875 1922\n",
      "loss: 42480.70703125 1923\n",
      "loss: 42472.2890625 1924\n",
      "loss: 42463.875 1925\n",
      "loss: 42455.484375 1926\n",
      "loss: 42447.06640625 1927\n",
      "loss: 42438.66015625 1928\n",
      "loss: 42430.2421875 1929\n",
      "loss: 42421.83984375 1930\n",
      "loss: 42413.4375 1931\n",
      "loss: 42405.0234375 1932\n",
      "loss: 42396.62890625 1933\n",
      "loss: 42388.20703125 1934\n",
      "loss: 42379.81640625 1935\n",
      "loss: 42371.41796875 1936\n",
      "loss: 42363.00390625 1937\n",
      "loss: 42354.6015625 1938\n",
      "loss: 42346.203125 1939\n",
      "loss: 42337.796875 1940\n",
      "loss: 42329.40625 1941\n",
      "loss: 42321.0078125 1942\n",
      "loss: 42312.6171875 1943\n",
      "loss: 42304.20703125 1944\n",
      "loss: 42295.80859375 1945\n",
      "loss: 42287.421875 1946\n",
      "loss: 42279.015625 1947\n",
      "loss: 42270.61328125 1948\n",
      "loss: 42262.21875 1949\n",
      "loss: 42253.82421875 1950\n",
      "loss: 42245.42578125 1951\n",
      "loss: 42237.0390625 1952\n",
      "loss: 42228.6328125 1953\n",
      "loss: 42220.25 1954\n",
      "loss: 42211.859375 1955\n",
      "loss: 42203.453125 1956\n",
      "loss: 42195.0703125 1957\n",
      "loss: 42186.66796875 1958\n",
      "loss: 42178.28515625 1959\n",
      "loss: 42169.890625 1960\n",
      "loss: 42161.4921875 1961\n",
      "loss: 42153.0859375 1962\n",
      "loss: 42144.7109375 1963\n",
      "loss: 42136.3125 1964\n",
      "loss: 42127.921875 1965\n",
      "loss: 42119.5234375 1966\n",
      "loss: 42111.140625 1967\n",
      "loss: 42102.74609375 1968\n",
      "loss: 42094.3515625 1969\n",
      "loss: 42085.96875 1970\n",
      "loss: 42077.5703125 1971\n",
      "loss: 42069.19140625 1972\n",
      "loss: 42060.7890625 1973\n",
      "loss: 42052.390625 1974\n",
      "loss: 42044.0 1975\n",
      "loss: 42035.61328125 1976\n",
      "loss: 42027.21875 1977\n",
      "loss: 42018.83203125 1978\n",
      "loss: 42010.4375 1979\n",
      "loss: 42002.0390625 1980\n",
      "loss: 41993.66796875 1981\n",
      "loss: 41985.26953125 1982\n",
      "loss: 41976.8828125 1983\n",
      "loss: 41968.46875 1984\n",
      "loss: 41960.0859375 1985\n",
      "loss: 41951.703125 1986\n",
      "loss: 41943.30078125 1987\n",
      "loss: 41934.91015625 1988\n",
      "loss: 41926.515625 1989\n",
      "loss: 41918.125 1990\n",
      "loss: 41909.73046875 1991\n",
      "loss: 41901.3359375 1992\n",
      "loss: 41892.9375 1993\n",
      "loss: 41884.5546875 1994\n",
      "loss: 41876.1484375 1995\n",
      "loss: 41867.7578125 1996\n",
      "loss: 41859.36328125 1997\n",
      "loss: 41850.9765625 1998\n",
      "loss: 41842.578125 1999\n",
      "loss: 41834.1875 2000\n",
      "loss: 41825.7890625 2001\n",
      "loss: 41817.39453125 2002\n",
      "loss: 41808.9921875 2003\n",
      "loss: 41800.59375 2004\n",
      "loss: 41792.203125 2005\n",
      "loss: 41783.796875 2006\n",
      "loss: 41775.3984375 2007\n",
      "loss: 41767.0 2008\n",
      "loss: 41758.609375 2009\n",
      "loss: 41750.1953125 2010\n",
      "loss: 41741.8203125 2011\n",
      "loss: 41733.41015625 2012\n",
      "loss: 41725.015625 2013\n",
      "loss: 41716.6171875 2014\n",
      "loss: 41708.21484375 2015\n",
      "loss: 41699.8046875 2016\n",
      "loss: 41691.4140625 2017\n",
      "loss: 41683.0078125 2018\n",
      "loss: 41674.61328125 2019\n",
      "loss: 41666.1953125 2020\n",
      "loss: 41657.796875 2021\n",
      "loss: 41649.390625 2022\n",
      "loss: 41640.984375 2023\n",
      "loss: 41632.59375 2024\n",
      "loss: 41624.18359375 2025\n",
      "loss: 41615.77734375 2026\n",
      "loss: 41607.37109375 2027\n",
      "loss: 41598.9609375 2028\n",
      "loss: 41590.5546875 2029\n",
      "loss: 41582.14453125 2030\n",
      "loss: 41573.734375 2031\n",
      "loss: 41565.33203125 2032\n",
      "loss: 41556.9140625 2033\n",
      "loss: 41548.5078125 2034\n",
      "loss: 41540.1015625 2035\n",
      "loss: 41531.6953125 2036\n",
      "loss: 41523.2734375 2037\n",
      "loss: 41514.859375 2038\n",
      "loss: 41506.4453125 2039\n",
      "loss: 41498.03515625 2040\n",
      "loss: 41489.62109375 2041\n",
      "loss: 41481.19921875 2042\n",
      "loss: 41472.7890625 2043\n",
      "loss: 41464.375 2044\n",
      "loss: 41455.953125 2045\n",
      "loss: 41447.5390625 2046\n",
      "loss: 41439.1171875 2047\n",
      "loss: 41430.6953125 2048\n",
      "loss: 41422.27734375 2049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 41413.859375 2050\n",
      "loss: 41405.4375 2051\n",
      "loss: 41397.0234375 2052\n",
      "loss: 41388.58984375 2053\n",
      "loss: 41380.1640625 2054\n",
      "loss: 41371.74609375 2055\n",
      "loss: 41363.32421875 2056\n",
      "loss: 41354.90625 2057\n",
      "loss: 41346.46875 2058\n",
      "loss: 41338.04296875 2059\n",
      "loss: 41329.609375 2060\n",
      "loss: 41321.1875 2061\n",
      "loss: 41312.74609375 2062\n",
      "loss: 41304.3203125 2063\n",
      "loss: 41295.89453125 2064\n",
      "loss: 41287.4609375 2065\n",
      "loss: 41279.0390625 2066\n",
      "loss: 41270.6015625 2067\n",
      "loss: 41262.171875 2068\n",
      "loss: 41253.7265625 2069\n",
      "loss: 41245.2890625 2070\n",
      "loss: 41236.859375 2071\n",
      "loss: 41228.421875 2072\n",
      "loss: 41219.9765625 2073\n",
      "loss: 41211.5390625 2074\n",
      "loss: 41203.1015625 2075\n",
      "loss: 41194.65625 2076\n",
      "loss: 41186.22265625 2077\n",
      "loss: 41177.77734375 2078\n",
      "loss: 41169.33203125 2079\n",
      "loss: 41160.8828125 2080\n",
      "loss: 41152.4453125 2081\n",
      "loss: 41143.99609375 2082\n",
      "loss: 41135.546875 2083\n",
      "loss: 41127.109375 2084\n",
      "loss: 41118.65234375 2085\n",
      "loss: 41110.203125 2086\n",
      "loss: 41101.7578125 2087\n",
      "loss: 41093.3046875 2088\n",
      "loss: 41084.8515625 2089\n",
      "loss: 41076.39453125 2090\n",
      "loss: 41067.9375 2091\n",
      "loss: 41059.48046875 2092\n",
      "loss: 41051.03125 2093\n",
      "loss: 41042.57421875 2094\n",
      "loss: 41034.11328125 2095\n",
      "loss: 41025.66015625 2096\n",
      "loss: 41017.203125 2097\n",
      "loss: 41008.7421875 2098\n",
      "loss: 41000.2734375 2099\n",
      "loss: 40991.80078125 2100\n",
      "loss: 40983.3515625 2101\n",
      "loss: 40974.88671875 2102\n",
      "loss: 40966.4140625 2103\n",
      "loss: 40957.94921875 2104\n",
      "loss: 40949.4921875 2105\n",
      "loss: 40941.015625 2106\n",
      "loss: 40932.54296875 2107\n",
      "loss: 40924.0859375 2108\n",
      "loss: 40915.61328125 2109\n",
      "loss: 40907.1328125 2110\n",
      "loss: 40898.65625 2111\n",
      "loss: 40890.1796875 2112\n",
      "loss: 40881.71484375 2113\n",
      "loss: 40873.234375 2114\n",
      "loss: 40864.75390625 2115\n",
      "loss: 40856.265625 2116\n",
      "loss: 40847.7890625 2117\n",
      "loss: 40839.30859375 2118\n",
      "loss: 40830.8359375 2119\n",
      "loss: 40822.34765625 2120\n",
      "loss: 40813.8671875 2121\n",
      "loss: 40805.375 2122\n",
      "loss: 40796.8984375 2123\n",
      "loss: 40788.40625 2124\n",
      "loss: 40779.9140625 2125\n",
      "loss: 40771.4140625 2126\n",
      "loss: 40762.9296875 2127\n",
      "loss: 40754.4453125 2128\n",
      "loss: 40745.94140625 2129\n",
      "loss: 40737.453125 2130\n",
      "loss: 40728.9453125 2131\n",
      "loss: 40720.44921875 2132\n",
      "loss: 40711.9609375 2133\n",
      "loss: 40703.46484375 2134\n",
      "loss: 40694.9609375 2135\n",
      "loss: 40686.46875 2136\n",
      "loss: 40677.9765625 2137\n",
      "loss: 40669.4609375 2138\n",
      "loss: 40660.96484375 2139\n",
      "loss: 40652.4453125 2140\n",
      "loss: 40643.9375 2141\n",
      "loss: 40635.4375 2142\n",
      "loss: 40626.921875 2143\n",
      "loss: 40618.421875 2144\n",
      "loss: 40609.90234375 2145\n",
      "loss: 40601.390625 2146\n",
      "loss: 40592.875 2147\n",
      "loss: 40584.359375 2148\n",
      "loss: 40575.8515625 2149\n",
      "loss: 40567.33984375 2150\n",
      "loss: 40558.8046875 2151\n",
      "loss: 40550.30078125 2152\n",
      "loss: 40541.7734375 2153\n",
      "loss: 40533.24609375 2154\n",
      "loss: 40524.7265625 2155\n",
      "loss: 40516.2109375 2156\n",
      "loss: 40507.6796875 2157\n",
      "loss: 40499.15234375 2158\n",
      "loss: 40490.609375 2159\n",
      "loss: 40482.109375 2160\n",
      "loss: 40473.5625 2161\n",
      "loss: 40465.03515625 2162\n",
      "loss: 40456.49609375 2163\n",
      "loss: 40447.9609375 2164\n",
      "loss: 40439.421875 2165\n",
      "loss: 40430.8984375 2166\n",
      "loss: 40422.3515625 2167\n",
      "loss: 40413.8203125 2168\n",
      "loss: 40405.265625 2169\n",
      "loss: 40396.7421875 2170\n",
      "loss: 40388.19140625 2171\n",
      "loss: 40379.640625 2172\n",
      "loss: 40371.1015625 2173\n",
      "loss: 40362.55859375 2174\n",
      "loss: 40354.0078125 2175\n",
      "loss: 40345.45703125 2176\n",
      "loss: 40336.90625 2177\n",
      "loss: 40328.3515625 2178\n",
      "loss: 40319.79296875 2179\n",
      "loss: 40311.23828125 2180\n",
      "loss: 40302.6953125 2181\n",
      "loss: 40294.13671875 2182\n",
      "loss: 40285.578125 2183\n",
      "loss: 40277.0078125 2184\n",
      "loss: 40268.453125 2185\n",
      "loss: 40259.8984375 2186\n",
      "loss: 40251.328125 2187\n",
      "loss: 40242.7578125 2188\n",
      "loss: 40234.1875 2189\n",
      "loss: 40225.6171875 2190\n",
      "loss: 40217.0625 2191\n",
      "loss: 40208.4921875 2192\n",
      "loss: 40199.921875 2193\n",
      "loss: 40191.34375 2194\n",
      "loss: 40182.7734375 2195\n",
      "loss: 40174.203125 2196\n",
      "loss: 40165.6328125 2197\n",
      "loss: 40157.046875 2198\n",
      "loss: 40148.453125 2199\n",
      "loss: 40139.8828125 2200\n",
      "loss: 40131.2890625 2201\n",
      "loss: 40122.71875 2202\n",
      "loss: 40114.125 2203\n",
      "loss: 40105.5390625 2204\n",
      "loss: 40096.94921875 2205\n",
      "loss: 40088.3515625 2206\n",
      "loss: 40079.765625 2207\n",
      "loss: 40071.1875 2208\n",
      "loss: 40062.58984375 2209\n",
      "loss: 40053.9921875 2210\n",
      "loss: 40045.390625 2211\n",
      "loss: 40036.796875 2212\n",
      "loss: 40028.19921875 2213\n",
      "loss: 40019.6015625 2214\n",
      "loss: 40010.99609375 2215\n",
      "loss: 40002.390625 2216\n",
      "loss: 39993.78125 2217\n",
      "loss: 39985.1796875 2218\n",
      "loss: 39976.58203125 2219\n",
      "loss: 39967.96875 2220\n",
      "loss: 39959.359375 2221\n",
      "loss: 39950.7421875 2222\n",
      "loss: 39942.12890625 2223\n",
      "loss: 39933.515625 2224\n",
      "loss: 39924.90625 2225\n",
      "loss: 39916.28125 2226\n",
      "loss: 39907.6640625 2227\n",
      "loss: 39899.046875 2228\n",
      "loss: 39890.41796875 2229\n",
      "loss: 39881.8046875 2230\n",
      "loss: 39873.1796875 2231\n",
      "loss: 39864.54296875 2232\n",
      "loss: 39855.9296875 2233\n",
      "loss: 39847.296875 2234\n",
      "loss: 39838.671875 2235\n",
      "loss: 39830.03515625 2236\n",
      "loss: 39821.3984375 2237\n",
      "loss: 39812.765625 2238\n",
      "loss: 39804.12890625 2239\n",
      "loss: 39795.5 2240\n",
      "loss: 39786.859375 2241\n",
      "loss: 39778.2109375 2242\n",
      "loss: 39769.5703125 2243\n",
      "loss: 39760.9140625 2244\n",
      "loss: 39752.28515625 2245\n",
      "loss: 39743.6328125 2246\n",
      "loss: 39734.98828125 2247\n",
      "loss: 39726.34375 2248\n",
      "loss: 39717.6875 2249\n",
      "loss: 39709.0234375 2250\n",
      "loss: 39700.375 2251\n",
      "loss: 39691.7265625 2252\n",
      "loss: 39683.0703125 2253\n",
      "loss: 39674.3984375 2254\n",
      "loss: 39665.74609375 2255\n",
      "loss: 39657.08203125 2256\n",
      "loss: 39648.42578125 2257\n",
      "loss: 39639.75 2258\n",
      "loss: 39631.09375 2259\n",
      "loss: 39622.4375 2260\n",
      "loss: 39613.7578125 2261\n",
      "loss: 39605.078125 2262\n",
      "loss: 39596.4140625 2263\n",
      "loss: 39587.734375 2264\n",
      "loss: 39579.0625 2265\n",
      "loss: 39570.3828125 2266\n",
      "loss: 39561.7109375 2267\n",
      "loss: 39553.03125 2268\n",
      "loss: 39544.34375 2269\n",
      "loss: 39535.6640625 2270\n",
      "loss: 39526.984375 2271\n",
      "loss: 39518.28515625 2272\n",
      "loss: 39509.609375 2273\n",
      "loss: 39500.9140625 2274\n",
      "loss: 39492.234375 2275\n",
      "loss: 39483.53125 2276\n",
      "loss: 39474.83984375 2277\n",
      "loss: 39466.1484375 2278\n",
      "loss: 39457.4453125 2279\n",
      "loss: 39448.734375 2280\n",
      "loss: 39440.046875 2281\n",
      "loss: 39431.34765625 2282\n",
      "loss: 39422.640625 2283\n",
      "loss: 39413.9375 2284\n",
      "loss: 39405.2265625 2285\n",
      "loss: 39396.51953125 2286\n",
      "loss: 39387.8125 2287\n",
      "loss: 39379.10546875 2288\n",
      "loss: 39370.390625 2289\n",
      "loss: 39361.671875 2290\n",
      "loss: 39352.9609375 2291\n",
      "loss: 39344.234375 2292\n",
      "loss: 39335.5234375 2293\n",
      "loss: 39326.8046875 2294\n",
      "loss: 39318.0859375 2295\n",
      "loss: 39309.359375 2296\n",
      "loss: 39300.6328125 2297\n",
      "loss: 39291.89453125 2298\n",
      "loss: 39283.16796875 2299\n",
      "loss: 39274.44140625 2300\n",
      "loss: 39265.7109375 2301\n",
      "loss: 39256.98046875 2302\n",
      "loss: 39248.25390625 2303\n",
      "loss: 39239.515625 2304\n",
      "loss: 39230.7734375 2305\n",
      "loss: 39222.0390625 2306\n",
      "loss: 39213.2890625 2307\n",
      "loss: 39204.5390625 2308\n",
      "loss: 39195.796875 2309\n",
      "loss: 39187.0546875 2310\n",
      "loss: 39178.30078125 2311\n",
      "loss: 39169.5625 2312\n",
      "loss: 39160.8046875 2313\n",
      "loss: 39152.04296875 2314\n",
      "loss: 39143.2890625 2315\n",
      "loss: 39134.54296875 2316\n",
      "loss: 39125.77734375 2317\n",
      "loss: 39117.0234375 2318\n",
      "loss: 39108.25 2319\n",
      "loss: 39099.5078125 2320\n",
      "loss: 39090.7265625 2321\n",
      "loss: 39081.96484375 2322\n",
      "loss: 39073.19140625 2323\n",
      "loss: 39064.4375 2324\n",
      "loss: 39055.6640625 2325\n",
      "loss: 39046.88671875 2326\n",
      "loss: 39038.1015625 2327\n",
      "loss: 39029.34375 2328\n",
      "loss: 39020.5546875 2329\n",
      "loss: 39011.76953125 2330\n",
      "loss: 39002.99609375 2331\n",
      "loss: 38994.203125 2332\n",
      "loss: 38985.42578125 2333\n",
      "loss: 38976.6328125 2334\n",
      "loss: 38967.84765625 2335\n",
      "loss: 38959.0546875 2336\n",
      "loss: 38950.2734375 2337\n",
      "loss: 38941.46875 2338\n",
      "loss: 38932.671875 2339\n",
      "loss: 38923.89453125 2340\n",
      "loss: 38915.078125 2341\n",
      "loss: 38906.28125 2342\n",
      "loss: 38897.48828125 2343\n",
      "loss: 38888.68359375 2344\n",
      "loss: 38879.87890625 2345\n",
      "loss: 38871.078125 2346\n",
      "loss: 38862.2734375 2347\n",
      "loss: 38853.453125 2348\n",
      "loss: 38844.640625 2349\n",
      "loss: 38835.8359375 2350\n",
      "loss: 38827.015625 2351\n",
      "loss: 38818.1953125 2352\n",
      "loss: 38809.37890625 2353\n",
      "loss: 38800.5546875 2354\n",
      "loss: 38791.7421875 2355\n",
      "loss: 38782.9140625 2356\n",
      "loss: 38774.09375 2357\n",
      "loss: 38765.2578125 2358\n",
      "loss: 38756.42578125 2359\n",
      "loss: 38747.59375 2360\n",
      "loss: 38738.7734375 2361\n",
      "loss: 38729.9375 2362\n",
      "loss: 38721.1015625 2363\n",
      "loss: 38712.26171875 2364\n",
      "loss: 38703.421875 2365\n",
      "loss: 38694.5703125 2366\n",
      "loss: 38685.75 2367\n",
      "loss: 38676.8984375 2368\n",
      "loss: 38668.046875 2369\n",
      "loss: 38659.19921875 2370\n",
      "loss: 38650.34375 2371\n",
      "loss: 38641.5 2372\n",
      "loss: 38632.6484375 2373\n",
      "loss: 38623.7890625 2374\n",
      "loss: 38614.9453125 2375\n",
      "loss: 38606.0703125 2376\n",
      "loss: 38597.2109375 2377\n",
      "loss: 38588.3515625 2378\n",
      "loss: 38579.4921875 2379\n",
      "loss: 38570.6171875 2380\n",
      "loss: 38561.7578125 2381\n",
      "loss: 38552.890625 2382\n",
      "loss: 38544.0234375 2383\n",
      "loss: 38535.14453125 2384\n",
      "loss: 38526.28125 2385\n",
      "loss: 38517.390625 2386\n",
      "loss: 38508.5234375 2387\n",
      "loss: 38499.6484375 2388\n",
      "loss: 38490.76171875 2389\n",
      "loss: 38481.8828125 2390\n",
      "loss: 38472.984375 2391\n",
      "loss: 38464.09375 2392\n",
      "loss: 38455.2109375 2393\n",
      "loss: 38446.3203125 2394\n",
      "loss: 38437.4296875 2395\n",
      "loss: 38428.5390625 2396\n",
      "loss: 38419.65234375 2397\n",
      "loss: 38410.7421875 2398\n",
      "loss: 38401.84375 2399\n",
      "loss: 38392.9453125 2400\n",
      "loss: 38384.0390625 2401\n",
      "loss: 38375.1484375 2402\n",
      "loss: 38366.234375 2403\n",
      "loss: 38357.3203125 2404\n",
      "loss: 38348.41796875 2405\n",
      "loss: 38339.5 2406\n",
      "loss: 38330.59375 2407\n",
      "loss: 38321.66796875 2408\n",
      "loss: 38312.7578125 2409\n",
      "loss: 38303.82421875 2410\n",
      "loss: 38294.90625 2411\n",
      "loss: 38285.98828125 2412\n",
      "loss: 38277.0625 2413\n",
      "loss: 38268.140625 2414\n",
      "loss: 38259.21875 2415\n",
      "loss: 38250.2890625 2416\n",
      "loss: 38241.3515625 2417\n",
      "loss: 38232.4296875 2418\n",
      "loss: 38223.484375 2419\n",
      "loss: 38214.546875 2420\n",
      "loss: 38205.609375 2421\n",
      "loss: 38196.66015625 2422\n",
      "loss: 38187.71875 2423\n",
      "loss: 38178.78125 2424\n",
      "loss: 38169.828125 2425\n",
      "loss: 38160.890625 2426\n",
      "loss: 38151.93359375 2427\n",
      "loss: 38142.984375 2428\n",
      "loss: 38134.02734375 2429\n",
      "loss: 38125.078125 2430\n",
      "loss: 38116.1171875 2431\n",
      "loss: 38107.1640625 2432\n",
      "loss: 38098.203125 2433\n",
      "loss: 38089.234375 2434\n",
      "loss: 38080.265625 2435\n",
      "loss: 38071.30859375 2436\n",
      "loss: 38062.3359375 2437\n",
      "loss: 38053.3671875 2438\n",
      "loss: 38044.3828125 2439\n",
      "loss: 38035.421875 2440\n",
      "loss: 38026.44921875 2441\n",
      "loss: 38017.4609375 2442\n",
      "loss: 38008.4921875 2443\n",
      "loss: 37999.5 2444\n",
      "loss: 37990.52734375 2445\n",
      "loss: 37981.5390625 2446\n",
      "loss: 37972.55078125 2447\n",
      "loss: 37963.5625 2448\n",
      "loss: 37954.57421875 2449\n",
      "loss: 37945.578125 2450\n",
      "loss: 37936.578125 2451\n",
      "loss: 37927.59375 2452\n",
      "loss: 37918.5859375 2453\n",
      "loss: 37909.59765625 2454\n",
      "loss: 37900.58984375 2455\n",
      "loss: 37891.57421875 2456\n",
      "loss: 37882.5703125 2457\n",
      "loss: 37873.5703125 2458\n",
      "loss: 37864.56640625 2459\n",
      "loss: 37855.5546875 2460\n",
      "loss: 37846.5390625 2461\n",
      "loss: 37837.53125 2462\n",
      "loss: 37828.515625 2463\n",
      "loss: 37819.5 2464\n",
      "loss: 37810.4765625 2465\n",
      "loss: 37801.44921875 2466\n",
      "loss: 37792.4140625 2467\n",
      "loss: 37783.390625 2468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 37774.3671875 2469\n",
      "loss: 37765.3359375 2470\n",
      "loss: 37756.30859375 2471\n",
      "loss: 37747.265625 2472\n",
      "loss: 37738.234375 2473\n",
      "loss: 37729.1953125 2474\n",
      "loss: 37720.15625 2475\n",
      "loss: 37711.1171875 2476\n",
      "loss: 37702.078125 2477\n",
      "loss: 37693.03125 2478\n",
      "loss: 37683.98828125 2479\n",
      "loss: 37674.93359375 2480\n",
      "loss: 37665.8828125 2481\n",
      "loss: 37656.84375 2482\n",
      "loss: 37647.78125 2483\n",
      "loss: 37638.7109375 2484\n",
      "loss: 37629.6640625 2485\n",
      "loss: 37620.59375 2486\n",
      "loss: 37611.5390625 2487\n",
      "loss: 37602.484375 2488\n",
      "loss: 37593.4140625 2489\n",
      "loss: 37584.34765625 2490\n",
      "loss: 37575.26171875 2491\n",
      "loss: 37566.20703125 2492\n",
      "loss: 37557.125 2493\n",
      "loss: 37548.04296875 2494\n",
      "loss: 37538.9609375 2495\n",
      "loss: 37529.8828125 2496\n",
      "loss: 37520.8046875 2497\n",
      "loss: 37511.7265625 2498\n",
      "loss: 37502.64453125 2499\n",
      "Final loss: 37502.64453125 2500\n",
      "Initial Training\n",
      "3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-47d32c120560>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mOutList1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrun_time1_ML\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMLGMRES\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mb_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mErr_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mProbIdx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrestart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrefine1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mblist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreslist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIterErrList10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGmresRunTime\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# normalize b for optimal NN performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mOut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOutList1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mErr_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOutList1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/NERSC/GMRES-Learning/src_dir/util.py\u001b[0m in \u001b[0;36mwrapper_timer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mrun_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m  \u001b[0;31m# 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/NERSC/GMRES-Learning/src_dir/nn_predictor.py\u001b[0m in \u001b[0;36mspeedup_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrefine\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m                 \u001b[0mIterErr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m                 \u001b[0mIterTime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoc\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0mIterErr10\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIterErr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/NERSC/GMRES-Learning/src_dir/util.py\u001b[0m in \u001b[0;36mresid\u001b[0;34m(A, *args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mA_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmatmul_a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mresid_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresid_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/NERSC/GMRES-Learning/src_dir/util.py\u001b[0m in \u001b[0;36mresid_kernel\u001b[0;34m(A, x, b)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mresid_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     return np.array(\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     )\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/NERSC/GMRES-Learning/src_dir/util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mresid_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     return np.array(\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     )\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/NERSC/GMRES-Learning/src_dir/util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mresid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mA_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmatmul_a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresid_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/NERSC/GMRES-Learning/src_dir/util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# changing data types.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmat_to_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m    \u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmatmul_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmat_to_a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/NERSC/GMRES-Learning/src_dir/util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# type for vector data. NOTE: this might cause a performance hit due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# changing data types.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmat_to_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m    \u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mmatmul_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmat_to_a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msqueeze\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msqueeze\u001b[0;34m(a, axis)\u001b[0m\n\u001b[1;32m   1430\u001b[0m     \"\"\"\n\u001b[1;32m   1431\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1432\u001b[0;31m         \u001b[0msqueeze\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1433\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'squeeze'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src_dir import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Err_list=[]\n",
    "IterErrList10=[]\n",
    "GmresRunTime=[]\n",
    "\n",
    "NonML_Err_List=[]\n",
    "\n",
    "\n",
    "debug=True\n",
    "\n",
    "refine1=False\n",
    "refine2=True\n",
    "\n",
    "run_time=0.0\n",
    "run_time_ML=0.0\n",
    "trainTime_total=0.0\n",
    "forwardTime_Total=0.0\n",
    "run_time_ML_NoAdd=0.0\n",
    "\n",
    "blist=[]\n",
    "reslist=[]\n",
    "\n",
    "\n",
    "\n",
    "for ProbIdx in range(n_steps):\n",
    "\n",
    "    loc=4*np.sin(0.5*ProbIdx*np.abs(np.cos(0.5*ProbIdx)))\n",
    "    b=Gauss_pdf(x,loc,sigma)\n",
    "    b_norm=np.linalg.norm(b)\n",
    "\n",
    "\n",
    "\n",
    "    OutList1,run_time1_ML=MLGMRES(A, b/b_norm, x0, e1, nmax_iter,Err_list,ProbIdx,restart,debug,refine1,blist,reslist,IterErrList10,GmresRunTime)  # normalize b for optimal NN performance.\n",
    "    Out=OutList1[0]\n",
    "    Err_list=OutList1[1]\n",
    "    IterErr0_AVG=OutList1[2]\n",
    "    trainTime1=OutList1[3]\n",
    "    forwardTime1=OutList1[4]\n",
    "    blist=OutList1[5]\n",
    "    reslist=OutList1[6]\n",
    "    IterErrList10=OutList1[7]\n",
    "    GmresRunTime=OutList1[8]\n",
    "\n",
    "    OutList2,run_time2_ML=MLGMRES(A, b/b_norm, Out[-1], e2, nmax_iter,Err_list,ProbIdx,restart,debug,refine2,blist,reslist,IterErrList10,GmresRunTime)\n",
    "    Out2=OutList2[0]\n",
    "    Err_list=OutList2[1]\n",
    "    IterErr0_AVG=OutList2[2]\n",
    "    trainTime2=OutList2[3]\n",
    "    forwardTime2=OutList2[4]\n",
    "    blist=OutList2[5]\n",
    "    reslist=OutList2[6]\n",
    "    IterErrList10=OutList2[7]\n",
    "    GmresRunTime=OutList2[8]\n",
    "\n",
    "    forwardTime_Total=forwardTime1+forwardTime2+forwardTime_Total\n",
    "    trainTime_total=trainTime2+trainTime1+trainTime_total\n",
    "    run_time_ML=run_time1_ML+run_time2_ML+run_time_ML\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    NonML_Out1,run_time1=GMRES_timed(A, b/b_norm, x0, e1, nmax_iter,restart, debug)  # normalize b for optimal NN performance.\n",
    "    NonML_Err=resid(A, NonML_Out1, b/b_norm)\n",
    "    NonML_Err_List.append(NonML_Err[10])\n",
    "    \n",
    "    NonML_Out2,run_time2=GMRES_timed(A, b/b_norm, NonML_Out1[-1], e2, nmax_iter,restart, debug)  # normalize b for optimal NN performance.\n",
    "    run_time=run_time1+run_time2+run_time\n",
    "    \n",
    "    print(ProbIdx)\n",
    "\n",
    "\n",
    "logger.info(\"Runtime of Non-decorated version is:\")\n",
    "logger.info(run_time)\n",
    "\n",
    "logger.info(\"Runtime of MLGMRES decorator is:\")\n",
    "logger.info(run_time_ML)\n",
    "\n",
    "\n",
    "logger.info(\"Runtime of training (backprop) is:\")\n",
    "logger.info(trainTime_total)\n",
    "\n",
    "logger.info(\"Runtime of forward function is:\")\n",
    "logger.info(forwardTime_Total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pp\n",
    "from src_dir import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVG=np.zeros((500,1))\n",
    "Err_Array=np.asarray(NonML_Err_List)\n",
    "count=np.arange(0,500)\n",
    "for j in range(0,500):\n",
    "    AVG[j]=moving_average(np.asarray(Err_Array[:j]),j)\n",
    "\n",
    "    \n",
    "    \n",
    "pp.plot(count,np.asarray(NonML_Err_List),'or',count[10:-1],AVG[10:-1],'oy')\n",
    "pp.xlabel('$i$')\n",
    "pp.ylabel('2-norm of residual of 10th iteration')\n",
    "pp.title('$n=600$ GMRES')\n",
    "pp.savefig('GMRES.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Err_Array_ML=np.asarray(IterErrList10)\n",
    "AVGML=np.zeros((500,1))\n",
    "\n",
    "for j in range(0,500):\n",
    "    AVGML[j]=moving_average(np.asarray(Err_Array_ML[:j]),j)\n",
    "pp.plot(count,np.asarray(Err_Array_ML),'ob',count,AVGML,'og')\n",
    "pp.xlabel('$i$')\n",
    "pp.ylabel('2-norm of residual of 10th iteration')\n",
    "pp.title('$n=600$ MLGMRES')\n",
    "pp.savefig('MLGMRES.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plot(count,Err_Array_ML,'ob',label='MLGMRES error')\n",
    "pp.plot(count,AVGML,'ok',label='Average MLGMRES error')\n",
    "pp.plot(count,Err_Array,'or',label='GMRES error')\n",
    "pp.plot(count[10:-1],AVG[10:-1],'oy',label='Average GMRES error')\n",
    "\n",
    "pp.xlabel('$i$')\n",
    "pp.ylabel('2-norm of residual of 10th iteration')\n",
    "pp.title('Error as a function of $b_i$ iteration ')\n",
    "pp.legend(loc='best')\n",
    "pp.savefig('Compare.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
