{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we provide a small example of our methodology to use deep learning in real time to accelerate GMRES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrate our real-time deep-learning methodology using the $2D$ Poisson problem. \n",
    "\n",
    "\n",
    "In particular, we solve a sequence of problems\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta u = f_i \\\\\n",
    "u |_{ \\partial \\Omega} = 0\n",
    "\\end{align} \n",
    "where $f_i$ are randomly permuted for every index. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The gist of our idea is that our real-time deep learning methodology wraps around a provided GMRES solver, and over time\n",
    "learns an initial guess/left perconditioner that accelerates the rate of convergence of the solver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set a number of things such as:\n",
    "- Dimension of $N \\times N$ grid\n",
    "- Default initial guess for GMRES iterations\n",
    "- Tolerance for GMRES solver\n",
    "- Restarted GMRES restart parameters\n",
    "- Total number of \"time steps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src_dir import mk_laplace_2d, mk_laplace_2d_Tensor\n",
    "import numpy as np\n",
    "\n",
    "# Set dimension of the NxN grid used\n",
    "# Note: For optimal performance, the neural network \"cnn_collectionOnline2D.py\"\n",
    "# can be tweaked with appropriate kernel dilations, however the code should\n",
    "# still work and yield resuluts for any dimension of input provided\n",
    "dim = 20\n",
    "\n",
    "# Default initial guess used for direct un-preconditioned GMRES is the zero\n",
    "# solution\n",
    "x0 = np.squeeze(np.zeros((dim,dim)))\n",
    "x0Type = 'Zero Solution 2D'\n",
    "\n",
    "# Set tolerances for GMRES solver\n",
    "e = 1e-5\n",
    "\n",
    "# Restarted GMRES parameters\n",
    "nmax_iter = 100\n",
    "restart   = 10000\n",
    "\n",
    "# Create domain [-1,1]x[-1,1]\n",
    "# Define grid values at midpoints of cartesian grid\n",
    "DomainL = -1.0\n",
    "DomainR =  1.0\n",
    "dx = (DomainR-DomainL)/(dim-1)\n",
    "x1 = np.linspace(DomainL+dx,DomainR-dx,dim)\n",
    "x2 = np.linspace(DomainL+dx,DomainR-dx,dim)\n",
    "X, Y = np.meshgrid(x1, x2, sparse=False, indexing='ij')\n",
    "Area = (dx*(dim-1))**2\n",
    "\n",
    "# Create 2D laplace opertor as a stencil opertor for a N-cell 2D grid\n",
    "# Can be found in linop.py in src_di\n",
    "A = mk_laplace_2d(dim, dim, bc=\"dirichlet\", xlo=0, xhi=0, ylo=0, yhi=0)\n",
    "AType = '2D Laplacian'\n",
    "\n",
    "# Total number of steps in simulation\n",
    "n_steps = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample of the RHS used in the demo is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sample of the RHS used in the demo\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "import matplotlib.pyplot as pp\n",
    "from mpl_toolkits.mplot3d import Axes3D  \n",
    "from src_dir import Gauss_pdf_2D\n",
    "\n",
    "# Compute dipole\n",
    "xloc      = np.random.uniform(x1[0], x1[-1])\n",
    "yloc      = np.random.uniform(x2[0], x2[-1])\n",
    "xlocShift = np.random.uniform(-0.25, 0.25)\n",
    "ylocShift = np.random.uniform(-0.25, 0.25)\n",
    "AmplitudeFactor  = np.random.uniform(1,10)\n",
    "AmplitudeFactor2 = AmplitudeFactor*np.random.uniform(1,2)\n",
    "sigma = 0.07*np.random.uniform(0.9,1.1)\n",
    "b     = AmplitudeFactor*Gauss_pdf_2D(X, Y, xloc, yloc, sigma) \\\n",
    "      + AmplitudeFactor2*Gauss_pdf_2D(X, Y, xloc + xlocShift, yloc + ylocShift, sigma)\n",
    "\n",
    "# Compute random field\n",
    "# Field = np.random.normal(loc=0.0, scale=1.0, size=(dim,dim))\n",
    "Field = AmplitudeFactor*np.random.normal(loc=0.0, scale=1.0, size=(dim, dim))\n",
    "\n",
    "b = b + Field\n",
    "\n",
    "pp.contourf(X,Y,b)\n",
    "pp.colorbar()\n",
    "\n",
    "fig = pp.figure()\n",
    "ax = pp.axes(projection='3d')\n",
    "ax.plot_surface(X, Y, b, cmap=pp.cm.jet, rstride=1, cstride=1, linewidth=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we wrap the simple unprecondition GMRES algorithm implemented in `gmres.py` using a wrapper\n",
    "that implements our deep learning methodology. This wrapper contains both the neural network we\n",
    "implemented, the training loop of this network, and additional data curration steps to work online\n",
    "\n",
    "The neural network can be found in `cnn_collectionOnline2D.py`, and the wrapper can be found in \n",
    "`cnn_predictorOnline2D.py`. Stay tuned for a paper the describes our methodology in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src_dir import cnn_preconditionerOnline_timed_2D, CNNPredictorOnline_2D,\\\n",
    "                    timer, GMRES, PreconditionerTrainer, FluidNet2D20\n",
    "\n",
    "# Note: # Model dimention inputs are not used for the current network in cnn_predictorOnline2D.py (but must be passed into wrapper)\n",
    "InputDim  = dim\n",
    "OutputDim = dim\n",
    "# Number of samples to collect before using preduction from Neural Network:\n",
    "Initial_set = 32\n",
    "\n",
    "nn_precon = CNNPredictorOnline_2D(InputDim, OutputDim, Area, dx, FluidNet2D20)\n",
    "# TODO: using diagnostic_probe=22 => end of first inner loop for the current setup => generalize this\n",
    "trainer   = PreconditionerTrainer(nn_precon, Initial_set=Initial_set)\n",
    "\n",
    "@timer\n",
    "@cnn_preconditionerOnline_timed_2D(trainer)\n",
    "def MLGMRES(A, b, x0, e, nmax_iter, restart, debug):\n",
    "    return GMRES(A, b, x0, e, nmax_iter, restart, debug)\n",
    "\n",
    "\n",
    "@timer\n",
    "def GMRES_timed(A, b, x0, e, nmax_iter, restart, debug):\n",
    "    return GMRES(A, b, x0, e, nmax_iter, restart, debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the performance of our approach on a series of poisson problems, we compare the wall clock time between the ML-wrapper GMRES implementation and the direct GMRES implementation.  The loop below (that loops for the number of steps set above) essentially has the following structure:\n",
    "\n",
    "1. Generate RHS\n",
    "2. Compute GMRES assisted by neural network (MLGMRES) up to tolreance $e$ and collected data.\n",
    "3. Compute direct GMRES up to $e$\n",
    "4. Write error metrics (such as time to solution and error at certain GMRES iterations) and go to 1.\n",
    "\n",
    "\n",
    "Scroll past the outputs to see some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src_dir import Gauss_pdf_2D, resid, StatusPrinter\n",
    "\n",
    "#Set the numpy seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Initialize lists that hold time data (time-to-solutuin, trainining time,\n",
    "# MLGMRES time, etc)\n",
    "run_time_ML_list     = []\n",
    "GmresRunTimeOriginal = []\n",
    "SpeedUp              = []\n",
    "trainTime_list       = []\n",
    "\n",
    "# Set debug mode (prints more information to screen)\n",
    "debug = True\n",
    "\n",
    "NonML_Err_List      = []\n",
    "NonML_Err_List_Full = []\n",
    "\n",
    "# Index of  Poisson problems solved\n",
    "for ProbIdx in range(n_steps):\n",
    "\n",
    "    # Set RHS of Poisson problem\n",
    "    xloc      = np.random.uniform(x1[0], x1[-1])\n",
    "    yloc      = np.random.uniform(x2[0], x2[-1])\n",
    "    xlocShift = np.random.uniform(-0.25, 0.25)\n",
    "    ylocShift = np.random.uniform(-0.25, 0.25)\n",
    "    AmplitudeFactor  = np.random.uniform(0.01, 10)\n",
    "    AmplitudeFactor2 = AmplitudeFactor*np.random.uniform(1, 2)\n",
    "    sigma = 0.07*np.random.uniform(0.9, 1.1)\n",
    "    b     = AmplitudeFactor*Gauss_pdf_2D(X, Y, xloc, yloc, sigma) \\\n",
    "          + AmplitudeFactor2*Gauss_pdf_2D(X, Y, xloc + xlocShift, yloc + ylocShift, sigma)\n",
    "    # Field = np.random.normal(loc=0.0, scale=1.0, size=(dim,dim))\n",
    "    Field = AmplitudeFactor*np.random.normal(loc=0.0, scale=1.0, size=(dim, dim))\n",
    "    b = b + Field\n",
    "\n",
    "    # b=np.maximum(xloc*2*X*np.sin(ProbIdx),yloc*2*Y*np.cos(ProbIdx))  # Linear gradient example\n",
    "    b = b*(dx**2.0) # Finite difference grid spacing\n",
    "\n",
    "    # First GMRES call (solve up to e1 tolerance) with ML wrapper\n",
    "    trainer.ProbCount = ProbIdx  # TODO: This should probably be automatically incremented\n",
    "    Out, run_time1_ML = MLGMRES(A, b, x0, e, nmax_iter, restart, debug)\n",
    "\n",
    "    # Collect ML assisted Run-times\n",
    "    run_time_ML_list.append(run_time1_ML)\n",
    "    if len(trainer.trainTime) > 0:\n",
    "        trainTime_list.append(trainer.trainTime[-1]) # TODO: this second list is not needed\n",
    "    \n",
    "    # Direct GMRES call up to e1 tolerance\n",
    "    NonML_Out1,run_time1 = GMRES_timed(A, b, x0, e, nmax_iter, restart, debug)  \n",
    "    NonML_Err = resid(A, NonML_Out1, b)\n",
    "    NonML_Err_List_Full.append(NonML_Err)\n",
    "    NonML_Err_List.append(NonML_Err[22])\n",
    "\n",
    "    ## Collect  direct GMRES time\n",
    "    GmresRunTimeOriginal.append(run_time1)\n",
    "\n",
    "    ## Ratio of run-times\n",
    "    SpeedUp.append(run_time1/trainer.ML_GMRES_Time_list[-1])\n",
    "    \n",
    "    # Update user on status\n",
    "    StatusPrinter().update_simulation(SpeedUp[-1], ProbIdx)\n",
    "\n",
    "\n",
    "StatusPrinter().finalize()\n",
    "\n",
    "\n",
    "MLGMRES_GMRES_ONLY = sum(trainer.ML_GMRES_Time_list)\n",
    "run_time           = sum(GmresRunTimeOriginal)\n",
    "run_time_ML        = sum(run_time_ML_list)\n",
    "trainTime_total    = sum(trainTime_list)\n",
    "\n",
    "\n",
    "print(\"Runtime of Non-decorated version is: \",     run_time)\n",
    "print(\"Runtime of MLGMRES decorator is: \",         run_time_ML)\n",
    "print(\"Runtime of MLGMRES (only GMRES time) is: \", MLGMRES_GMRES_ONLY)\n",
    "print(\"Runtime of training (backprop) is: \",       trainTime_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the error for the 5th GMRES iteration for every \"time-step\"  is plotted. The gist is that we see neural network is learning to produce initial guesses that improve the error at this iteration as our simulation progresses.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pp\n",
    "from src_dir import moving_average\n",
    "\n",
    "# Compute moving average of GMRES and MLGMRES error\n",
    "AVG   = np.zeros((n_steps, 1))\n",
    "count = np.arange(0, n_steps)\n",
    "\n",
    "Err_Array = np.asarray(NonML_Err_List)\n",
    "count     = np.arange(0, n_steps)\n",
    "for j in range(0, n_steps):\n",
    "    AVG[j] = moving_average(np.asarray(Err_Array[:j]), j)\n",
    "\n",
    "Err_Array_ML = np.asarray(trainer.Err_list)\n",
    "AVGML        = np.zeros((n_steps, 1))\n",
    "for j in range(0, n_steps):\n",
    "    AVGML[j] = moving_average(np.asarray(Err_Array_ML[:j]), j)    \n",
    "\n",
    "# Compute moving average of GMRES and MLGMRES run-times\n",
    "GmresRunTimeOriginal_AVG = np.zeros((n_steps, 1))\n",
    "ML_GMRES_Time_AVG        = np.zeros((n_steps, 1))\n",
    "\n",
    "for j in range(0, n_steps):\n",
    "    GmresRunTimeOriginal_AVG[j] = moving_average(np.asarray(GmresRunTimeOriginal[:j]), j)\n",
    "\n",
    "for j in range(0, n_steps):\n",
    "    ML_GMRES_Time_AVG[j] = moving_average(np.asarray(trainer.ML_GMRES_Time_list[:j]), j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plot(count,Err_Array_ML,'.b',label='MLGMRES error')\n",
    "pp.plot(count[10:-1],AVGML[10:-1],'k',label='Average MLGMRES error')\n",
    "pp.plot(count,Err_Array,'.r',label='GMRES error')\n",
    "pp.plot(count[10:-1],AVG[10:-1],'g',label='Average GMRES error')\n",
    "\n",
    "pp.xlabel('$i$')\n",
    "pp.ylabel('$||r_2||_2$')\n",
    "pp.title('Error as a function of $i$-th iteration')\n",
    "pp.legend(loc='best')\n",
    "pp.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the time-to-solution for every \"time-step\"  is plotted.\n",
    "Here, we see the neural network is learning to produce initial guesses that improve the overall time-to-solution for every iteration as our simulation progresses.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.plot(trainer.ML_GMRES_Time_list,'.b',label='MLGMRES')\n",
    "pp.plot(GmresRunTimeOriginal,'.r', label='GMRES')\n",
    "pp.plot(count[10:-1],GmresRunTimeOriginal_AVG[10:-1],'g', label='GMRES Average')\n",
    "pp.plot(count[10:-1],ML_GMRES_Time_AVG[10:-1],'k', label='MLGMRES Average')\n",
    "\n",
    "pp.ylabel('Time (s)')\n",
    "pp.xlabel('i')\n",
    "pp.title('GMRES run time')\n",
    "# pp.legend(loc='best')\n",
    "# pp.ylim(0.025,0.15)\n",
    "pp.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure below, the GMRES iterations for a single linear problem is plotted. This figures is particularly useful for understanding why we are getting a speed-up at all. In a nut-shell, we get a speed up because the NN-provided intial guess temporarily improves the rate of convergence of GMRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RHSIndex=-1\n",
    "pp.semilogy(NonML_Err_List_Full[RHSIndex],'.r',label='GMRES')\n",
    "pp.semilogy(trainer.IterErrList[RHSIndex],'.b',label='MLGMRES ')\n",
    "pp.legend(loc='best')\n",
    "pp.xlabel('GMRES iterations')\n",
    "pp.ylabel('$||r||_2$')\n",
    "pp.title('Convergence of Algorithim for Final Linear Problem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio of GMRES/MLGMRES time-to-solutions are plotted below for the first $e_1$ tolerance. \n",
    "It should be noted that our methodology wraps around the direct GMRES solver,\n",
    "so the exact same solver is used in both approaches, and only differ by what initial condition\n",
    "(or effectively left-preconditioner) is applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMRESAVG=GmresRunTimeOriginal_AVG[10:-1]\n",
    "MLGMRESAVG=ML_GMRES_Time_AVG[10:-1]\n",
    "Ratio=np.divide(GMRESAVG,MLGMRESAVG)\n",
    "\n",
    "pp.plot(Ratio,'.b')\n",
    "pp.xlabel('i')\n",
    "pp.ylabel('GMRES/MLGMRES')\n",
    "pp.title(\"NN Speed Up \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
