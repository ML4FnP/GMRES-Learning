{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on terminology "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are subtle differences between online, continual, and incremental learning\n",
    "\n",
    "\n",
    "- The terminology is not particularly uniform or settled\n",
    "\n",
    "\n",
    "- The state of the art in these method (usually under the title of continual/continuous learning) seek to address the situation where the output of the neural networks may change as more data is collected. Since out output is constant for a given run (i.e we desire an output vector of a fixed size, we do not need to get too sophisticated. So, good \"search terms\" that fit our use are \"online\" and \"incremental\" learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note after the DAS/DSEG intern meeting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At some point during the meeting, I explained the underlining idea of our plans and desire to use online learning \n",
    "\n",
    "\n",
    "- They suggest we first directly try  traditional methods(i.e non-online, non-incremential, etc) on our \"goal problem\" first and show that solution data can actually speed up convergence before  trying to use more non-standard modalities.\n",
    " \n",
    " \n",
    "- Generally, they saw the path forward for us as either trying to emphasize depth in the project(jumping right to the \"goal\" problem), or breadth (exploring alternate approaches such as online methods first).\n",
    "\n",
    "\n",
    "\n",
    "- A personal note: I feel an intresting clash in methodologies here. Coming from a  physics/math background, we often start with \"toy\" problems to build intuition and build up from there. However, the inclination (at least in this group of data scientists) is to just jump into the deep end and throw whatever standard methods that may work, and then tweak things to really beat down the given problem.\n",
    "\n",
    "\n",
    "- A personal note: The \"in-situ\"\"realtime\" learning aspect of our problem was either not emphasized enough by me, or sort of overlooked by the suggestions given. It seems that the inclination was to immediately turn me back toward off-line learning. However, as we've seen, the ability to train quickly and effeciently is essential. I credit this misunderstanding to the fact that we're straying from the usual data science workflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Research Papers skimmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online Deep Learning: Learning Deep Neural Networks on the Fly: https://arxiv.org/pdf/1711.03705.pdf\n",
    "\n",
    "\n",
    "- Paper proposes  iteratively constructing a deeper network using a modified back propogation approach. Ultimately, the approach is too complicated for our relatively simple problem at hand\n",
    "\n",
    "\n",
    "- Has a decent and brief overview of online methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Progress & Compress: A scalable framework for continual learning\n",
    "\n",
    "https://arxiv.org/abs/1805.06370\n",
    "\n",
    "-  As the title implies, this paper seeks to address continual learning in a scalable way\n",
    "\n",
    "- This paper essentially seeks to refine ideas that have been proposed in the past. For example, regularization approaches that penalize changing of weights to prevent forgetting can impare learning of new data.\n",
    "\n",
    "- Among things, this paper proposes an online version of EWC ( a regularization method to prevent forgetting). Some ideas used here may transfer to our interested scenario. There is a pytorch implementation of this online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review papers skimmed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online Learning and Stochastic Approximations (2012): https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjl_8mw3NLpAhWMo54KHaOpCisQFjABegQIAxAB&url=https%3A%2F%2Fwww.cs.huji.ac.il%2F~shais%2Fpapers%2FOLsurvey.pdf&usg=AOvVaw0YA_MW2PUuxmC4dfA3Cdoc\n",
    "\n",
    "- A rigorous(theorem-proof structure) overview  of optimization methods for online learning. (Essentially a technical version of the online learning wikipedia page). Not clear how these methods mesh with deep learning as we need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online Learning and Stochastic Approximations (1998)\n",
    "\n",
    "https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjyqdrp2dLpAhXF6Z4KHaBJAfIQFjABegQIBBAB&url=https%3A%2F%2Fleon.bottou.org%2Fpublications%2Fpdf%2Fonline-1998.pdf&usg=AOvVaw3qK7TS52Rn59BxmanXnz1g\\n\n",
    "\n",
    "\n",
    "- A fairly dated overview of online optimization methods. Nontheless some useful formulations of online gradient descent  algorithms (and other types as well) and applications are present here. In hindsight, it makes sense that ML researchers once really cared about this stuff. Storage was even more of an issue not too long ago....\n",
    "\n",
    "\n",
    "- This survey as well as the one above seem to be the defacto standard references in modern papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continual Lifelong Learning with Neural Networks: A Review (2019)\n",
    "\n",
    "https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjy5vLM1NTpAhUK7J4KHSDWD7wQFjABegQIAhAB&url=https%3A%2F%2Farxiv.org%2Fabs%2F1802.07569&usg=AOvVaw3fG4Wd89-GiotIbsL6bA_z\n",
    "\n",
    "\n",
    "- A modern review of approaches for realtime learning type problems for neural networks\n",
    "\n",
    "\n",
    "- Covers ideas beyond the shuffling of old data with new data in gradient descent \n",
    "\n",
    "- Since continual learning addresses a harder problem (changing tasks as well as data), some methods here are far more intricate than what we need \n",
    "\n",
    "    Some ideas covered that may work for us:\n",
    "\n",
    "- Regularization approaches where we protect knowledge by penalizing changes in the weights\n",
    "- Dynamic architecture approaches where the depth on the neural network is dynamically changed as data is accumulated \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task-Free Continual Learning: https://arxiv.org/abs/1812.03596\n",
    "\n",
    "- proposes transforming a continual learning problem to an online learning problems with data distributions gradually changing and without the notion of separate tasks (similar to our situation)\n",
    "\n",
    "\n",
    "- We only deal with one task in our work, so the paper is not particularly applicable to our work.\n",
    "\n",
    "\n",
    "- Provides useful insight into how to think about online learning with neural networks and mitigating catastrophic inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Promising papers currently reading, but not yet read in desired detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some remarks on general online learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the papers I've read outline the following history of online methods\n",
    "\n",
    "- Online learning was first established for learning linear models\n",
    "\n",
    "- Online learning with Kernels were later introduced. They allow for shallow nonlinear models, but do not allow for much \"nonlinearity\"  \n",
    "\n",
    "- Frequently, many papers note that while neural networks are superior for creating nonlinear models, they seem to suffer from a variety of issues when considered in an online context. In particular,  convergence and  catastrophic inference(forgetting of previous samples when learning new ones)  are typical issues.  Many research papers attempt to try to solve these issues in some way"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
